import aiosqlite
import json
import asyncio
import redis
import time
import os
from collections import defaultdict
from threading import Lock
import hashlib
from developTools.utils.logger import get_logger
from run.ai_llm.service.aiReplyHandler.gemini import gemini_prompt_elements_construct
from run.ai_llm.service.aiReplyHandler.openai import prompt_elements_construct, prompt_elements_construct_old_version

DB_NAME = "data/dataBase/group_messages.db"


def is_running_in_docker():
    return os.path.exists("/.dockerenv") or os.environ.get("IN_DOCKER") == "1"


if is_running_in_docker():
    REDIS_URL = "redis://redis:6379/0"
else:
    REDIS_URL = "redis://localhost"

# ä¼˜åŒ–åçš„ç¼“å­˜é…ç½®
REDIS_CACHE_TTL = 300  # å¢åŠ åˆ°5åˆ†é’Ÿ
MEMORY_CACHE_TTL = 60  # å†…å­˜ç¼“å­˜1åˆ†é’Ÿ
BATCH_SIZE = 10  # æ‰¹é‡å†™å…¥å¤§å°

logger = get_logger()

redis_client = None

# å†…å­˜ç¼“å­˜å’Œæ‰¹é‡å†™å…¥
memory_cache = {}
cache_timestamps = {}
pending_writes = defaultdict(list)
write_lock = Lock()
last_batch_write = time.time()

import subprocess
import platform
import zipfile

REDIS_EXECUTABLE = "redis-server.exe"
REDIS_ZIP_PATH = os.path.join("data", "Redis-x64-5.0.14.1.zip")
REDIS_FOLDER = os.path.join("data", "redis_extracted")


def extract_redis_from_local_zip():
    """ä»æœ¬åœ° zip è§£å‹ Redis åˆ°æŒ‡å®šç›®å½•"""
    if not os.path.exists(REDIS_FOLDER):
        os.makedirs(REDIS_FOLDER)
        logger.info("ğŸ“¦ æ­£åœ¨ä»æœ¬åœ°å‹ç¼©åŒ…è§£å‹ Redis...")
        with zipfile.ZipFile(REDIS_ZIP_PATH, 'r') as zip_ref:
            zip_ref.extractall(REDIS_FOLDER)
        logger.info("âœ… Redis è§£å‹å®Œæˆ")


def start_redis_background():
    """åœ¨åå°å¯åŠ¨ Redisï¼ˆæ”¯æŒ Windows å’Œ Linuxï¼‰"""
    system = platform.system()
    extract_redis_from_local_zip()
    if system == "Windows":
        redis_path = os.path.join(REDIS_FOLDER, REDIS_EXECUTABLE)
        if not os.path.exists(redis_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ° redis-server.exe äº {redis_path}")
            return
        logger.info("ğŸš€ å¯åŠ¨ Redis æœåŠ¡ä¸­ (Windows)...")
        subprocess.Popen([redis_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    elif system == "Linux":
        try:
            logger.info("ğŸš€ å°è¯•åœ¨åå°å¯åŠ¨ Redis æœåŠ¡ (Linux)...")
            subprocess.Popen(["redis-server"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except FileNotFoundError:
            logger.error("âŒ 'redis-server' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿ Redis å·²å®‰è£…å¹¶åœ¨ç³»ç»Ÿçš„ PATH ä¸­ã€‚")
        except Exception as e:
            logger.error(f"âŒ åœ¨ Linux ä¸Šå¯åŠ¨ Redis å¤±è´¥: {e}")
    else:
        logger.warning(f"âš ï¸ ä¸æ”¯æŒåœ¨ {system} ç³»ç»Ÿä¸Šè‡ªåŠ¨å¯åŠ¨ Redisã€‚")


def init_redis():
    global redis_client
    if redis_client is not None:
        return
    try:
        # ä¼˜åŒ–Redisè¿æ¥é…ç½®
        redis_client = redis.StrictRedis.from_url(
            REDIS_URL,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
            health_check_interval=30
        )
        redis_client.ping()
        logger.info("âœ… Redis è¿æ¥æˆåŠŸï¼ˆæ•°æ®åº“ db groupï¼‰")
    except redis.exceptions.ConnectionError:
        logger.warning("âš ï¸ Redis æœªè¿è¡Œï¼Œå°è¯•è‡ªåŠ¨å¯åŠ¨ Redis...")
        system = platform.system()
        if system == "Windows" or system == "Linux":
            start_redis_background()
            time.sleep(2)
            try:
                redis_client = redis.StrictRedis.from_url(
                    REDIS_URL,
                    socket_connect_timeout=5,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )
                redis_client.ping()
                logger.info(f"âœ… Redis å·²åœ¨ {system} ä¸Šè‡ªåŠ¨å¯åŠ¨å¹¶è¿æ¥æˆåŠŸï¼ˆæ•°æ®åº“ db1ï¼‰")
            except Exception as e:
                logger.error(f"âŒ Redis è‡ªåŠ¨å¯åŠ¨åè¿æ¥å¤±è´¥ï¼š{e}")
                redis_client = None
        else:
            logger.error(f"âŒ é Windows/Linux ç³»ç»Ÿï¼Œè¯·æ‰‹åŠ¨å®‰è£…å¹¶å¯åŠ¨ Redis")
            redis_client = None


init_redis()


# ======================= ä¼˜åŒ–çš„ç¼“å­˜ç®¡ç† =======================
def get_cache_key(group_id: int, prompt_standard: str, data_length: int = 20):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"group:{group_id}:{prompt_standard}:{data_length}"


def get_memory_cache(key: str):
    """è·å–å†…å­˜ç¼“å­˜"""
    if key in memory_cache:
        timestamp = cache_timestamps.get(key, 0)
        if time.time() - timestamp < MEMORY_CACHE_TTL:
            return memory_cache[key]
        else:
            # è¿‡æœŸæ¸…ç†
            memory_cache.pop(key, None)
            cache_timestamps.pop(key, None)
    return None


def set_memory_cache(key: str, value):
    """è®¾ç½®å†…å­˜ç¼“å­˜"""
    memory_cache[key] = value
    cache_timestamps[key] = time.time()

    # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    if len(memory_cache) > 1000:
        current_time = time.time()
        expired_keys = [
            k for k, t in cache_timestamps.items()
            if current_time - t > MEMORY_CACHE_TTL
        ]
        for k in expired_keys:
            memory_cache.pop(k, None)
            cache_timestamps.pop(k, None)


def get_redis_cache(key: str):
    """å®‰å…¨è·å–Redisç¼“å­˜"""
    if not redis_client:
        return None
    try:
        cached = redis_client.get(key)
        return json.loads(cached) if cached else None
    except Exception as e:
        logger.debug(f"Redisè¯»å–å¤±è´¥: {e}")
        return None


def set_redis_cache(key: str, value, ttl: int = REDIS_CACHE_TTL):
    """å®‰å…¨è®¾ç½®Redisç¼“å­˜"""
    if not redis_client:
        return
    try:
        redis_client.setex(key, ttl, json.dumps(value))
    except Exception as e:
        logger.debug(f"Rediså†™å…¥å¤±è´¥: {e}")


def clear_redis_cache_pattern(pattern: str):
    """æ¸…ç†Redisç¼“å­˜æ¨¡å¼"""
    if not redis_client:
        return
    try:
        keys = redis_client.keys(pattern)
        if keys:
            redis_client.delete(*keys)
    except Exception as e:
        logger.debug(f"Redisæ¸…ç†å¤±è´¥: {e}")


# ======================= ä¼˜åŒ–çš„æ•°æ®åº“æ“ä½œ =======================
MAX_RETRIES = 3
INITIAL_DELAY = 0.1
CONNECTION_POOL = {}


async def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆè¿æ¥æ± ï¼‰"""
    thread_id = id(asyncio.current_task())
    if thread_id not in CONNECTION_POOL:
        db = await aiosqlite.connect(DB_NAME)
        # ä¼˜åŒ–æ•°æ®åº“è®¾ç½®
        await db.execute("PRAGMA journal_mode=WAL;")
        await db.execute("PRAGMA synchronous=NORMAL;")
        await db.execute("PRAGMA cache_size=10000;")
        await db.execute("PRAGMA temp_store=MEMORY;")
        await db.execute("PRAGMA busy_timeout=5000;")
        CONNECTION_POOL[thread_id] = db
    return CONNECTION_POOL[thread_id]


async def execute_with_retry(db, query, params=None):
    """ä¼˜åŒ–çš„å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œ"""
    for attempt in range(MAX_RETRIES):
        try:
            if params:
                cursor = await db.execute(query, params)
            else:
                cursor = await db.execute(query)
            return cursor
        except aiosqlite.OperationalError as e:
            if "database is locked" in str(e) or "busy" in str(e):
                delay = INITIAL_DELAY * (2 ** attempt) + (attempt * 0.05)  # æ›´çŸ­çš„é€€é¿æ—¶é—´
                logger.debug(f"Database busy, retrying in {delay:.3f}s (attempt {attempt + 1})")
                await asyncio.sleep(delay)
            else:
                raise
        except Exception as e:
            logger.error(f"Database error: {e}")
            raise
    raise Exception(f"Database still busy after {MAX_RETRIES} attempts")


# ======================= æ‰¹é‡å†™å…¥ä¼˜åŒ– =======================
async def batch_write_pending():
    """æ‰¹é‡å†™å…¥å¾…å¤„ç†çš„æ•°æ®"""
    global last_batch_write
    current_time = time.time()

    if current_time - last_batch_write < 1.0:  # 1ç§’å†…ä¸é‡å¤å†™å…¥
        return

    with write_lock:
        if not pending_writes:
            return

        batch_data = dict(pending_writes)
        pending_writes.clear()
        last_batch_write = current_time

    if not batch_data:
        return

    try:
        db = await get_db_connection()
        for group_id, messages in batch_data.items():
            if messages:
                # æ‰¹é‡æ’å…¥
                insert_data = [
                    (group_id, json.dumps(msg), None, None, None)
                    for msg in messages
                ]
                await db.executemany(
                    "INSERT INTO group_messages (group_id, message, processed_message, new_openai_processed_message, old_openai_processed_message) VALUES (?, ?, ?, ?, ?)",
                    insert_data
                )

                # æ¸…ç†æ—§æ•°æ®
                cursor = await db.execute("SELECT COUNT(*) FROM group_messages WHERE group_id = ?", (group_id,))
                count = (await cursor.fetchone())[0]

                if count > 50:
                    excess = count - 50
                    await execute_with_retry(
                        db,
                        "DELETE FROM group_messages WHERE id IN (SELECT id FROM group_messages WHERE group_id = ? ORDER BY timestamp ASC LIMIT ?)",
                        (group_id, excess)
                    )

        await db.commit()
        logger.debug(f"æ‰¹é‡å†™å…¥å®Œæˆ: {len(batch_data)} ä¸ªç¾¤ç»„")

        # æ¸…ç†ç›¸å…³ç¼“å­˜
        for group_id in batch_data.keys():
            clear_redis_cache_pattern(f"group:{group_id}:*")
            # æ¸…ç†å†…å­˜ç¼“å­˜
            expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k]
            for k in expired_keys:
                memory_cache.pop(k, None)
                cache_timestamps.pop(k, None)

    except Exception as e:
        logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")


# ======================= å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡ç®¡ç† =======================
from typing import Optional

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡å’Œåˆå§‹åŒ–çŠ¶æ€
_periodic_task: Optional[asyncio.Task] = None
_db_initialized: bool = False


# å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡
async def periodic_batch_write():
    """å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    while True:
        try:
            await asyncio.sleep(2)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
            await batch_write_pending()
        except Exception as e:
            logger.error(f"å®šæœŸæ‰¹é‡å†™å…¥é”™è¯¯: {e}")


def start_periodic_batch_write():
    """å¯åŠ¨å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            logger.info("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç¨åå†å¯åŠ¨
        logger.debug("æš‚æ—¶æ— æ³•å¯åŠ¨å®šæœŸä»»åŠ¡ï¼Œç­‰å¾…äº‹ä»¶å¾ªç¯å¯åŠ¨")


def stop_periodic_batch_write():
    """åœæ­¢å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    if _periodic_task and not _periodic_task.done():
        _periodic_task.cancel()
        logger.info("ğŸ›‘ å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²åœæ­¢")


def ensure_periodic_task():
    """ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            logger.debug("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¿½ç•¥
        pass


async def ensure_db_initialized():
    """ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–"""
    global _db_initialized
    if not _db_initialized:
        await init_db()
        _db_initialized = True


# ======================= åˆå§‹åŒ– =======================
async def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œæ£€æŸ¥å¹¶æ·»åŠ å¿…è¦çš„å­—æ®µ"""
    db = await get_db_connection()
    try:
        await execute_with_retry(db, """
            CREATE TABLE IF NOT EXISTS group_messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_id INTEGER NOT NULL,
                message TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                processed_message TEXT,
                new_openai_processed_message TEXT,
                old_openai_processed_message TEXT
            )
        """)

        # åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_timestamp ON group_messages(group_id, timestamp);")
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_id ON group_messages(group_id);")

        await db.commit()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # å¯åŠ¨å®šæœŸä»»åŠ¡
        start_periodic_batch_write()

    except Exception as e:
        logger.warning(f"Error initializing database: {e}")


# ======================= ä¼˜åŒ–çš„æ·»åŠ æ¶ˆæ¯ =======================
async def add_to_group(group_id: int, message, delete_after: int = 50):
    """å‘ç¾¤ç»„æ·»åŠ æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨æ‰¹é‡å†™å…¥ï¼‰"""
    init_redis()

    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ
    ensure_periodic_task()

    with write_lock:
        pending_writes[group_id].append(message)

        # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿçš„æ¶ˆæ¯ï¼Œç«‹å³å†™å…¥
        if len(pending_writes[group_id]) >= BATCH_SIZE:
            asyncio.create_task(batch_write_pending())


async def get_group_messages(group_id: int, limit: int = 50):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æŒ‡å®šæ•°é‡æ¶ˆæ¯ï¼Œä»…è¿”å›æ–‡æœ¬çš„åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    cache_key = f"messages:{group_id}:{limit}"
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    try:
        query = "SELECT message FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC"
        params = (group_id,)
        if limit is not None:
            query += " LIMIT ?"
            params += (limit,)

        db = await get_db_connection()
        cursor = await execute_with_retry(db, query, params)
        rows = await cursor.fetchall()

        text_list = []
        for row in rows:
            try:
                raw_message = json.loads(row[0])
                if "message" in raw_message and isinstance(raw_message["message"], list):
                    for msg_obj in raw_message["message"]:
                        if isinstance(msg_obj, dict) and "text" in msg_obj and isinstance(msg_obj["text"], str):
                            text_list.append(msg_obj["text"])
            except (json.JSONDecodeError, KeyError):
                pass

        # ç¼“å­˜ç»“æœ
        set_memory_cache(cache_key, text_list)
        return text_list

    except Exception as e:
        logger.info(f"Error getting messages for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„è·å–å¹¶è½¬æ¢æ¶ˆæ¯ =======================
async def get_last_20_and_convert_to_prompt(group_id: int, data_length=20, prompt_standard="gemini", bot=None,
                                            event=None):
    """è·å–æœ€è¿‘çš„æ¶ˆæ¯å¹¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼çš„ promptï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    init_redis()

    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    cache_key = get_cache_key(group_id, prompt_standard, data_length)

    # ä¸‰çº§ç¼“å­˜ï¼šå†…å­˜ -> Redis -> æ•°æ®åº“
    # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    # 2. æ£€æŸ¥Redisç¼“å­˜
    cached = get_redis_cache(cache_key)
    if cached:
        set_memory_cache(cache_key, cached)
        return cached

    # æ˜ å°„ä¸åŒçš„æ ‡å‡†å­—æ®µ
    field_mapping = {
        "gemini": "processed_message",
        "new_openai": "new_openai_processed_message",
        "old_openai": "old_openai_processed_message"
    }

    if prompt_standard not in field_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„ prompt_standard: {prompt_standard}")

    selected_field = field_mapping[prompt_standard]

    # 3. ä»æ•°æ®åº“è·å–
    try:
        # å…ˆç«‹å³å†™å…¥å¾…å¤„ç†çš„æ¶ˆæ¯
        await batch_write_pending()

        db = await get_db_connection()
        cursor = await execute_with_retry(
            db,
            f"SELECT id, message, {selected_field} FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC LIMIT ?",
            (group_id, data_length)
        )
        rows = await cursor.fetchall()

        final_list = []
        updates_needed = []  # æ”¶é›†éœ€è¦æ›´æ–°çš„æ•°æ®

        for row in rows:
            message_id, raw_message, processed_message = row
            raw_message = json.loads(raw_message)

            # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œä½¿ç”¨ç¼“å­˜çš„æ¶ˆæ¯
            if processed_message:
                final_list.append(json.loads(processed_message))
            else:
                raw_message["message"].insert(0, {
                    "text": f"æœ¬æ¡æ¶ˆæ¯æ¶ˆæ¯å‘é€è€…ä¸º {raw_message['user_name']} idä¸º{raw_message['user_id']} è¿™æ˜¯å‚è€ƒæ¶ˆæ¯ï¼Œå½“æˆ‘å†æ¬¡å‘ä½ æé—®æ—¶ï¼Œè¯·æ­£å¸¸å›å¤æˆ‘ã€‚"
                })

                if prompt_standard == "gemini":
                    processed = await gemini_prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                elif prompt_standard == "new_openai":
                    processed = await prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                    final_list.append(
                        {"role": "assistant", "content": [{"type": "text", "text": "(ç¾¤èŠèƒŒæ™¯æ¶ˆæ¯å·²è®°å½•)"}]})
                else:
                    processed = await prompt_elements_construct_old_version(raw_message["message"], bot=bot,
                                                                            event=event)
                    final_list.append(processed)
                    final_list.append({"role": "assistant", "content": "(ç¾¤èŠèƒŒæ™¯æ¶ˆæ¯å·²è®°å½•)"})

                # æ”¶é›†æ›´æ–°æ•°æ®
                updates_needed.append((json.dumps(processed), message_id, selected_field))

        # æ‰¹é‡æ›´æ–°æ•°æ®åº“
        if updates_needed:
            for processed_json, message_id, field in updates_needed:
                await execute_with_retry(
                    db,
                    f"UPDATE group_messages SET {field} = ? WHERE id = ?",
                    (processed_json, message_id)
                )
            await db.commit()

        # å¤„ç†æœ€ç»ˆæ ¼å¼åŒ–çš„æ¶ˆæ¯
        fl = []
        if prompt_standard == "gemini":
            all_parts = [part for entry in final_list if entry['role'] == 'user' for part in entry['parts']]
            fl.append({"role": "user", "parts": all_parts})
            fl.append({"role": "model", "parts": {"text": "å—¯å—¯ï¼Œæˆ‘è®°ä½äº†"}})
        else:
            all_parts = []
            all_parts_str = ""
            for entry in final_list:
                if entry['role'] == 'user':
                    if isinstance(entry['content'], str):
                        all_parts_str += entry['content'] + "\n"
                    else:
                        for part in entry['content']:
                            all_parts.append(part)
            fl.append({"role": "user", "content": all_parts if all_parts else all_parts_str})
            fl.append({"role": "assistant", "content": "å—¯å—¯æˆ‘è®°ä½äº†"})

        # è®¾ç½®ä¸‰çº§ç¼“å­˜
        set_memory_cache(cache_key, fl)
        set_redis_cache(cache_key, fl)

        return fl

    except Exception as e:
        logger.info(f"Error getting last 20 and converting to prompt for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„æ¸…é™¤æ¶ˆæ¯ =======================
async def clear_group_messages(group_id: int):
    """æ¸…é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    init_redis()

    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    try:
        # å…ˆæ¸…ç†å¾…å†™å…¥çš„æ•°æ®
        with write_lock:
            pending_writes.pop(group_id, None)

        db = await get_db_connection()
        await execute_with_retry(
            db,
            "DELETE FROM group_messages WHERE group_id = ?",
            (group_id,)
        )
        await db.commit()
        logger.info(f"âœ… å·²æ¸…é™¤ group_id={group_id} çš„æ‰€æœ‰æ•°æ®")

        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        clear_redis_cache_pattern(f"group:{group_id}:*")

        # æ¸…ç†å†…å­˜ç¼“å­˜
        expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k or f"messages:{group_id}:" in k]
        for k in expired_keys:
            memory_cache.pop(k, None)
            cache_timestamps.pop(k, None)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç† group_id={group_id} æ•°æ®æ—¶å‡ºé”™: {e}")


# ======================= æ€§èƒ½ç›‘æ§ =======================
def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "memory_cache_size": len(memory_cache),
        "pending_writes_groups": len(pending_writes),
        "pending_writes_total": sum(len(msgs) for msgs in pending_writes.values()),
        "redis_connected": redis_client is not None,
        "db_initialized": _db_initialized,
        "periodic_task_running": _periodic_task is not None and not _periodic_task.done()
    }