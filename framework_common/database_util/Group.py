import aiosqlite
import json
import asyncio
import time
import os
import threading
import weakref
import gc
from collections import defaultdict, OrderedDict, deque
from threading import Lock
import hashlib
from developTools.utils.logger import get_logger
from framework_common.database_util.RedisCacheManager import create_group_cache_manager
from run.ai_llm.service.aiReplyHandler.gemini import gemini_prompt_elements_construct
from run.ai_llm.service.aiReplyHandler.openai import prompt_elements_construct, prompt_elements_construct_old_version

# å¯¼å…¥Redisç¼“å­˜ç®¡ç†å™¨


DB_NAME = "data/dataBase/group_messages.db"

# ä¼˜åŒ–åçš„ç¼“å­˜é…ç½®
REDIS_CACHE_TTL = 250  # å¢åŠ åˆ°5åˆ†é’Ÿ
MEMORY_CACHE_TTL = 50  # å†…å­˜ç¼“å­˜1åˆ†é’Ÿ
BATCH_SIZE = 10  # æ‰¹é‡å†™å…¥å¤§å°

logger = get_logger()

# ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨ (æ•°æ®åº“0)
redis_cache = create_group_cache_manager(cache_ttl=REDIS_CACHE_TTL)


# ======================= ä¿®å¤å†…å­˜ç¼“å­˜ç®¡ç† =======================
class LRUMemoryCache:
    """LRUå†…å­˜ç¼“å­˜ï¼Œé˜²æ­¢æ— é™å¢é•¿"""

    def __init__(self, max_size=500, ttl=50):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.timestamps = {}
        self.lock = threading.Lock()

    def get(self, key):
        with self.lock:
            if key in self.cache:
                if time.time() - self.timestamps[key] < self.ttl:
                    self.cache.move_to_end(key)  # LRUæ›´æ–°
                    return self.cache[key]
                else:
                    # è¿‡æœŸæ¸…ç†
                    del self.cache[key]
                    del self.timestamps[key]
            return None

    def set(self, key, value):
        with self.lock:
            current_time = time.time()

            if key in self.cache:
                self.cache.move_to_end(key)
            else:
                # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œç§»é™¤æœ€è€çš„é¡¹
                if len(self.cache) >= self.max_size:
                    oldest = next(iter(self.cache))
                    del self.cache[oldest]
                    del self.timestamps[oldest]

            self.cache[key] = value
            self.timestamps[key] = current_time

    def pop(self, key, default=None):
        with self.lock:
            self.timestamps.pop(key, None)
            return self.cache.pop(key, default)

    def keys(self):
        with self.lock:
            return list(self.cache.keys())

    def __len__(self):
        with self.lock:
            return len(self.cache)

    def clear(self):
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()

    def cleanup_expired(self):
        """æ‰‹åŠ¨æ¸…ç†è¿‡æœŸé¡¹"""
        with self.lock:
            current_time = time.time()
            expired_keys = [
                k for k, t in self.timestamps.items()
                if current_time - t > self.ttl
            ]
            for k in expired_keys:
                self.cache.pop(k, None)
                self.timestamps.pop(k, None)
            return len(expired_keys)


# ä½¿ç”¨LRUç¼“å­˜æ›¿ä»£åŸæœ‰çš„å­—å…¸
memory_cache = LRUMemoryCache(max_size=500, ttl=MEMORY_CACHE_TTL)


# ======================= ä¿®å¤æ‰¹é‡å†™å…¥æ•°æ®ç®¡ç† =======================
class BoundedPendingWrites:
    """æœ‰ç•Œçš„å¾…å†™å…¥æ•°æ®ç®¡ç†ï¼Œé˜²æ­¢æ— é™ç´¯ç§¯"""

    def __init__(self, max_per_group=100):
        self.max_per_group = max_per_group
        self.data = defaultdict(lambda: deque(maxlen=max_per_group))
        self.lock = threading.Lock()

    def append(self, group_id, message):
        with self.lock:
            self.data[group_id].append(message)

    def get_and_clear_group(self, group_id):
        with self.lock:
            if group_id in self.data:
                messages = list(self.data[group_id])
                self.data[group_id].clear()
                return messages
            return []

    def clear_all(self):
        with self.lock:
            result = {}
            for group_id, messages in self.data.items():
                if messages:
                    result[group_id] = list(messages)
                    messages.clear()
            return result

    def is_empty(self):
        with self.lock:
            return not any(self.data.values())

    def get_group_size(self, group_id):
        with self.lock:
            return len(self.data.get(group_id, []))

    def __len__(self):
        with self.lock:
            return len(self.data)

    def total_messages(self):
        with self.lock:
            return sum(len(messages) for messages in self.data.values())


pending_writes = BoundedPendingWrites(max_per_group=100)
write_lock = Lock()
last_batch_write = time.time()

# ======================= ä¿®å¤æ•°æ®åº“è¿æ¥ç®¡ç† =======================
# ä¼˜åŒ–åçš„æ•°æ®åº“è¿æ¥ç®¡ç†
MAX_RETRIES = 3
INITIAL_DELAY = 0.1
_db_connection = None
_connection_lock = threading.Lock()


async def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œä¿®å¤è¿æ¥æ³„æ¼ï¼‰"""
    global _db_connection

    if _db_connection is None:
        with _connection_lock:
            if _db_connection is None:
                _db_connection = await aiosqlite.connect(DB_NAME)
                # ä¼˜åŒ–æ•°æ®åº“è®¾ç½®
                await _db_connection.execute("PRAGMA journal_mode=WAL;")
                await _db_connection.execute("PRAGMA synchronous=NORMAL;")
                await _db_connection.execute("PRAGMA cache_size=10000;")
                await _db_connection.execute("PRAGMA temp_store=MEMORY;")
                await _db_connection.execute("PRAGMA busy_timeout=5000;")

    return _db_connection


# è·Ÿè¸ªå¼‚æ­¥ä»»åŠ¡ï¼Œé˜²æ­¢ä»»åŠ¡æ³„æ¼
_running_tasks = weakref.WeakSet()


# ======================= ä¿æŒåŸæœ‰çš„ç¼“å­˜ç®¡ç†æ¥å£ =======================
def get_cache_key(group_id: int, prompt_standard: str, data_length: int = 20):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"group:{group_id}:{prompt_standard}:{data_length}"


def get_memory_cache(key: str):
    """è·å–å†…å­˜ç¼“å­˜ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰"""
    return memory_cache.get(key)


def set_memory_cache(key: str, value):
    """è®¾ç½®å†…å­˜ç¼“å­˜ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰"""
    memory_cache.set(key, value)


def get_redis_cache(key: str):
    """å®‰å…¨è·å–Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    return redis_cache.get(key)


def set_redis_cache(key: str, value, ttl: int = REDIS_CACHE_TTL):
    """å®‰å…¨è®¾ç½®Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    redis_cache.set(key, value, ttl)


def clear_redis_cache_pattern(pattern: str):
    """æ¸…ç†Redisç¼“å­˜æ¨¡å¼ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    redis_cache.delete_pattern(pattern)


# ======================= ä¼˜åŒ–çš„æ•°æ®åº“æ“ä½œ =======================
async def execute_with_retry(db, query, params=None):
    """ä¼˜åŒ–çš„å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œ"""
    for attempt in range(MAX_RETRIES):
        try:
            if params:
                cursor = await db.execute(query, params)
            else:
                cursor = await db.execute(query)
            return cursor
        except aiosqlite.OperationalError as e:
            if "database is locked" in str(e) or "busy" in str(e):
                delay = INITIAL_DELAY * (2 ** attempt) + (attempt * 0.05)  # æ›´çŸ­çš„é€€é¿æ—¶é—´
                logger.debug(f"Database busy, retrying in {delay:.3f}s (attempt {attempt + 1})")
                await asyncio.sleep(delay)
            else:
                raise
        except Exception as e:
            logger.error(f"Database error: {e}")
            raise
    raise Exception(f"Database still busy after {MAX_RETRIES} attempts")


# ======================= æ‰¹é‡å†™å…¥ä¼˜åŒ– =======================
async def batch_write_pending():
    """æ‰¹é‡å†™å…¥å¾…å¤„ç†çš„æ•°æ®ï¼ˆä¿®å¤æ•°æ®æ³„æ¼ï¼‰"""
    global last_batch_write
    current_time = time.time()

    if current_time - last_batch_write < 1.0:  # 1ç§’å†…ä¸é‡å¤å†™å…¥
        return

    # è·å–å¹¶æ¸…ç©ºå¾…å†™å…¥æ•°æ®
    batch_data = pending_writes.clear_all()
    if not batch_data:
        return

    last_batch_write = current_time

    try:
        db = await get_db_connection()
        for group_id, messages in batch_data.items():
            if messages:
                # æ‰¹é‡æ’å…¥
                insert_data = [
                    (group_id, json.dumps(msg), None, None, None)
                    for msg in messages
                ]
                await db.executemany(
                    "INSERT INTO group_messages (group_id, message, processed_message, new_openai_processed_message, old_openai_processed_message) VALUES (?, ?, ?, ?, ?)",
                    insert_data
                )

                # æ¸…ç†æ—§æ•°æ®
                cursor = await db.execute("SELECT COUNT(*) FROM group_messages WHERE group_id = ?", (group_id,))
                count = (await cursor.fetchone())[0]

                if count > 50:
                    excess = count - 50
                    await execute_with_retry(
                        db,
                        "DELETE FROM group_messages WHERE id IN (SELECT id FROM group_messages WHERE group_id = ? ORDER BY timestamp ASC LIMIT ?)",
                        (group_id, excess)
                    )

        await db.commit()
        # logger.debug(f"æ‰¹é‡å†™å…¥å®Œæˆ: {len(batch_data)} ä¸ªç¾¤ç»„")

        # æ¸…ç†ç›¸å…³ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        for group_id in batch_data.keys():
            clear_redis_cache_pattern(f"group:{group_id}:*")
            # æ¸…ç†å†…å­˜ç¼“å­˜
            expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k]
            for k in expired_keys:
                memory_cache.pop(k, None)

    except Exception as e:
        logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
        # å†™å…¥å¤±è´¥æ—¶ï¼Œå°†æ•°æ®é‡æ–°æ”¾å›é˜Ÿåˆ—ï¼ˆé¿å…æ•°æ®ä¸¢å¤±ï¼‰
        for group_id, messages in batch_data.items():
            for msg in messages:
                pending_writes.append(group_id, msg)


# ======================= å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡ç®¡ç† =======================
from typing import Optional

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡å’Œåˆå§‹åŒ–çŠ¶æ€
_periodic_task: Optional[asyncio.Task] = None
_db_initialized: bool = False


async def periodic_batch_write():
    """å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    while True:
        try:
            await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            await batch_write_pending()

            # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
            if time.time() % 60 < 5:  # å¤§çº¦æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                memory_cache.cleanup_expired()

        except Exception as e:
            logger.error(f"å®šæœŸæ‰¹é‡å†™å…¥é”™è¯¯: {e}")


def start_periodic_batch_write():
    """å¯åŠ¨å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            _running_tasks.add(_periodic_task)  # è·Ÿè¸ªä»»åŠ¡
            logger.info("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç¨åå†å¯åŠ¨
        logger.debug("æš‚æ—¶æ— æ³•å¯åŠ¨å®šæœŸä»»åŠ¡ï¼Œç­‰å¾…äº‹ä»¶å¾ªç¯å¯åŠ¨")


def stop_periodic_batch_write():
    """åœæ­¢å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    if _periodic_task and not _periodic_task.done():
        _periodic_task.cancel()
        logger.info("ğŸ›‘ å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²åœæ­¢")


def ensure_periodic_task():
    """ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            _running_tasks.add(_periodic_task)  # è·Ÿè¸ªä»»åŠ¡
            logger.debug("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¿½ç•¥
        pass


async def ensure_db_initialized():
    """ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–"""
    global _db_initialized
    if not _db_initialized:
        await init_db()
        _db_initialized = True


# ======================= åˆå§‹åŒ– =======================
async def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œæ£€æŸ¥å¹¶æ·»åŠ å¿…è¦çš„å­—æ®µ"""
    db = await get_db_connection()
    try:
        await execute_with_retry(db, """
            CREATE TABLE IF NOT EXISTS group_messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_id INTEGER NOT NULL,
                message TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                processed_message TEXT,
                new_openai_processed_message TEXT,
                old_openai_processed_message TEXT
            )
        """)

        # åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_timestamp ON group_messages(group_id, timestamp);")
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_id ON group_messages(group_id);")

        await db.commit()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # å¯åŠ¨å®šæœŸä»»åŠ¡
        start_periodic_batch_write()

    except Exception as e:
        logger.warning(f"Error initializing database: {e}")


# ======================= ä¼˜åŒ–çš„æ·»åŠ æ¶ˆæ¯ =======================
async def add_to_group(group_id: int, message, delete_after: int = 50):
    """å‘ç¾¤ç»„æ·»åŠ æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨æ‰¹é‡å†™å…¥ï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ
    ensure_periodic_task()

    pending_writes.append(group_id, message)

    # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿçš„æ¶ˆæ¯ï¼Œç«‹å³å†™å…¥
    if pending_writes.get_group_size(group_id) >= BATCH_SIZE:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ‰¹é‡å†™å…¥ä»»åŠ¡åœ¨è¿è¡Œ
        has_running_batch_task = any(
            not task.done() and hasattr(task, '_batch_write')
            for task in _running_tasks
        )

        if not has_running_batch_task:
            task = asyncio.create_task(batch_write_pending())
            task._batch_write = True  # æ ‡è®°ä»»åŠ¡ç±»å‹
            _running_tasks.add(task)


async def get_group_messages(group_id: int, limit: int = 50):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æŒ‡å®šæ•°é‡æ¶ˆæ¯ï¼Œä»…è¿”å›æ–‡æœ¬çš„åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    cache_key = f"messages:{group_id}:{limit}"
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    try:
        query = "SELECT message FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC"
        params = (group_id,)
        if limit is not None:
            query += " LIMIT ?"
            params += (limit,)

        db = await get_db_connection()
        cursor = await execute_with_retry(db, query, params)
        rows = await cursor.fetchall()

        text_list = []
        for row in rows:
            try:
                raw_message = json.loads(row[0])
                if "message" in raw_message and isinstance(raw_message["message"], list):
                    for msg_obj in raw_message["message"]:
                        if isinstance(msg_obj, dict) and "text" in msg_obj and isinstance(msg_obj["text"], str):
                            text_list.append(msg_obj["text"])
            except (json.JSONDecodeError, KeyError):
                pass

        # ç¼“å­˜ç»“æœ
        set_memory_cache(cache_key, text_list)
        return text_list

    except Exception as e:
        logger.info(f"Error getting messages for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„è·å–å¹¶è½¬æ¢æ¶ˆæ¯ =======================

async def get_last_20_and_convert_to_prompt(group_id: int, data_length=20, prompt_standard="gemini", bot=None,
                                            event=None):
    """è·å–æœ€è¿‘çš„æ¶ˆæ¯å¹¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼çš„ promptï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    cache_key = get_cache_key(group_id, prompt_standard, data_length)

    # ä¸‰çº§ç¼“å­˜ï¼šå†…å­˜ -> Redis -> æ•°æ®åº“
    # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    # 2. æ£€æŸ¥Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
    cached = get_redis_cache(cache_key)
    if cached:
        set_memory_cache(cache_key, cached)
        return cached

    # æ˜ å°„ä¸åŒçš„æ ‡å‡†å­—æ®µ
    field_mapping = {
        "gemini": "processed_message",
        "new_openai": "new_openai_processed_message",
        "old_openai": "old_openai_processed_message"
    }

    if prompt_standard not in field_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„ prompt_standard: {prompt_standard}")

    selected_field = field_mapping[prompt_standard]

    # 3. ä»æ•°æ®åº“è·å–
    try:
        # å…ˆç«‹å³å†™å…¥å¾…å¤„ç†çš„æ¶ˆæ¯
        await batch_write_pending()

        db = await get_db_connection()
        cursor = await execute_with_retry(
            db,
            f"SELECT id, message, {selected_field}, timestamp FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC LIMIT ?",
            (group_id, data_length)
        )
        rows = await cursor.fetchall()

        final_list = []
        updates_needed = []  # æ”¶é›†éœ€è¦æ›´æ–°çš„æ•°æ®

        # ç”¨äºæ„å»ºä¸Šä¸‹æ–‡æ‘˜è¦çš„ä¿¡æ¯ï¼ˆé™åˆ¶å¤§å°é˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        MAX_PARTICIPANTS = 20
        MAX_ACTIVITIES = 10

        context_info = {
            'participants': set(),
            'message_count': len(rows),
            'topics': [],
            'activities': []
        }

        for i, row in enumerate(rows):
            message_id, raw_message, processed_message, timestamp = row
            raw_message = json.loads(raw_message)

            # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆé™åˆ¶å¤§å°ï¼‰
            user_name = raw_message.get('user_name', 'æœªçŸ¥ç”¨æˆ·')
            user_id = raw_message.get('user_id', '')

            if len(context_info['participants']) < MAX_PARTICIPANTS:
                context_info['participants'].add(f"{user_name}(ID:{user_id})")

            # åˆ†ææ¶ˆæ¯å†…å®¹ç±»å‹
            message_content = raw_message.get("message", [])
            content_types = []
            for msg_part in message_content:
                if msg_part.get('type') == 'text':
                    text = msg_part.get('text', '').strip()
                    if text:
                        # ç®€å•çš„è¯é¢˜æå–ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
                        if '?' in text or 'ï¼Ÿ' in text:
                            if len(context_info['activities']) < MAX_ACTIVITIES:
                                context_info['activities'].append('æœ‰äººæé—®')
                        if any(word in text for word in ['å›¾ç‰‡', 'ç…§ç‰‡', 'çœ‹çœ‹']):
                            if len(context_info['activities']) < MAX_ACTIVITIES:
                                context_info['activities'].append('è®¨è®ºå›¾ç‰‡')
                        if any(word in text for word in ['æ–‡ä»¶', 'é“¾æ¥', 'http']):
                            if len(context_info['activities']) < MAX_ACTIVITIES:
                                context_info['activities'].append('åˆ†äº«æ–‡ä»¶/é“¾æ¥')
                elif msg_part.get('type') == 'image':
                    content_types.append('å›¾ç‰‡')
                elif msg_part.get('type') == 'file':
                    content_types.append('æ–‡ä»¶')
                elif msg_part.get('type') == 'audio':
                    content_types.append('è¯­éŸ³')
                elif msg_part.get('type') == 'video':
                    content_types.append('è§†é¢‘')

            if content_types and len(context_info['activities']) < MAX_ACTIVITIES:
                context_info['activities'].extend([f"å‘é€äº†{ct}" for ct in content_types[:3]])  # é™åˆ¶æ•°é‡

            # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œä½¿ç”¨ç¼“å­˜çš„æ¶ˆæ¯
            if processed_message:
                final_list.append(json.loads(processed_message))
            else:
                # æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æç¤ºä¿¡æ¯
                position_desc = "æœ€æ–°" if i == 0 else f"ç¬¬{i + 1}æ¡"

                context_prompt = (
                    f"ã€ç¾¤èŠä¸Šä¸‹æ–‡-{position_desc}æ¶ˆæ¯ã€‘"
                    f"å‘é€è€…ï¼š{user_name}(ID:{user_id}) | "
                    f"æ—¶é—´æˆ³ï¼š{timestamp} | "
                    f"æ¶ˆæ¯ä½ç½®ï¼šå€’æ•°ç¬¬{i + 1}æ¡"
                )

                raw_message["message"].insert(0, {
                    "text": f"{context_prompt}\nè¿™æ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ï¼Œç”¨äºç†è§£å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‚å½“æˆ‘å†æ¬¡å‘ä½ æé—®æ—¶ï¼Œè¯·ç»“åˆè¿™äº›ä¸Šä¸‹æ–‡ä¿¡æ¯æ­£å¸¸å›å¤æˆ‘ã€‚"
                })

                if prompt_standard == "gemini":
                    processed = await gemini_prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                elif prompt_standard == "new_openai":
                    processed = await prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                else:
                    processed = await prompt_elements_construct_old_version(raw_message["message"], bot=bot,
                                                                            event=event)
                    final_list.append(processed)

                # æ”¶é›†æ›´æ–°æ•°æ®
                updates_needed.append((json.dumps(processed), message_id, selected_field))

        # æ‰¹é‡æ›´æ–°æ•°æ®åº“
        if updates_needed:
            for processed_json, message_id, field in updates_needed:
                await execute_with_retry(
                    db,
                    f"UPDATE group_messages SET {field} = ? WHERE id = ?",
                    (processed_json, message_id)
                )
            await db.commit()

        # æ„å»ºç¾¤èŠæ¦‚å†µæ‘˜è¦
        participants_list = list(context_info['participants'])
        activities_summary = list(set(context_info['activities'])) if context_info['activities'] else ['æ­£å¸¸èŠå¤©']

        group_summary = (
            f"ã€ç¾¤èŠæ¦‚å†µã€‘å‚ä¸äººæ•°ï¼š{len(participants_list)}äºº | "
            f"æ¶ˆæ¯æ€»æ•°ï¼š{context_info['message_count']}æ¡ | "
            f"ä¸»è¦å‚ä¸è€…ï¼š{', '.join(participants_list[:5])}{'...' if len(participants_list) > 5 else ''} | "
            f"æ´»åŠ¨ç±»å‹ï¼š{', '.join(activities_summary[:3])}{'...' if len(activities_summary) > 3 else ''}"
        )

        # å¤„ç†æœ€ç»ˆæ ¼å¼åŒ–çš„æ¶ˆæ¯
        fl = []
        if prompt_standard == "gemini":
            all_parts = [part for entry in final_list if entry['role'] == 'user' for part in entry['parts']]

            # åœ¨å¼€å¤´æ·»åŠ ç¾¤èŠæ¦‚å†µ
            summary_part = {"text": f"{group_summary}\nä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ä½ ç†è§£å¯¹è¯èƒŒæ™¯ã€‚"}
            all_parts.insert(0, summary_part)

            fl.append({"role": "user", "parts": all_parts})
            fl.append({"role": "model", "parts": {
                "text": "æˆ‘å·²ç»äº†è§£äº†ç¾¤èŠçš„ä¸Šä¸‹æ–‡èƒŒæ™¯ï¼ŒåŒ…æ‹¬å‚ä¸æˆå‘˜ã€æ¶ˆæ¯å†å²å’Œä¸»è¦æ´»åŠ¨ã€‚æˆ‘ä¼šç»“åˆè¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°ç†è§£å’Œå›åº”åç»­çš„å¯¹è¯ã€‚"}})
        else:
            all_parts = []
            all_parts_str = f"{group_summary}\n"

            for entry in final_list:
                if entry['role'] == 'user':
                    if isinstance(entry['content'], str):
                        all_parts_str += entry['content'] + "\n"
                    else:
                        for part in entry['content']:
                            all_parts.append(part)

            if all_parts:
                # åœ¨å¼€å¤´æ·»åŠ æ¦‚å†µè¯´æ˜
                summary_part = {"type": "text", "text": f"{group_summary}\nä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼š"}
                all_parts.insert(0, summary_part)
                fl.append({"role": "user", "content": all_parts})
            else:
                fl.append({"role": "user", "content": all_parts_str + "ä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ã€‚"})

            fl.append({"role": "assistant",
                       "content": "æˆ‘å·²ç»äº†è§£äº†ç¾¤èŠçš„ä¸Šä¸‹æ–‡èƒŒæ™¯ï¼ŒåŒ…æ‹¬å‚ä¸æˆå‘˜ã€æ¶ˆæ¯å†å²å’Œä¸»è¦æ´»åŠ¨ã€‚æˆ‘ä¼šç»“åˆè¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°ç†è§£å’Œå›åº”åç»­çš„å¯¹è¯ã€‚"})

        # è®¾ç½®ä¸‰çº§ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        set_memory_cache(cache_key, fl)
        set_redis_cache(cache_key, fl)
        # print(fl)
        return fl

    except Exception as e:
        logger.info(f"Error getting last 20 and converting to prompt for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„æ¸…é™¤æ¶ˆæ¯ =======================
async def clear_group_messages(group_id: int):
    """æ¸…é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    try:
        # å…ˆæ¸…ç†å¾…å†™å…¥çš„æ•°æ®
        pending_writes.get_and_clear_group(group_id)

        db = await get_db_connection()
        await execute_with_retry(
            db,
            "DELETE FROM group_messages WHERE group_id = ?",
            (group_id,)
        )
        await db.commit()
        logger.info(f"âœ… å·²æ¸…é™¤ group_id={group_id} çš„æ‰€æœ‰æ•°æ®")

        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        clear_redis_cache_pattern(f"group:{group_id}:*")

        # æ¸…ç†å†…å­˜ç¼“å­˜
        expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k or f"messages:{group_id}:" in k]
        for k in expired_keys:
            memory_cache.pop(k, None)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç† group_id={group_id} æ•°æ®æ—¶å‡ºé”™: {e}")


# ======================= æ–°å¢ï¼šç¼“å­˜ç®¡ç†åŠŸèƒ½ =======================
async def clear_all_group_cache():
    """æ¸…é™¤æ‰€æœ‰ç¾¤ç»„ç›¸å…³çš„ç¼“å­˜"""
    try:
        # æ¸…é™¤Redisç¼“å­˜
        redis_cache.delete_pattern("group:*")
        redis_cache.delete_pattern("messages:*")

        # æ¸…é™¤å†…å­˜ç¼“å­˜
        memory_cache.clear()

        logger.info("âœ… æ‰€æœ‰ç¾¤ç»„ç¼“å­˜å·²æ¸…é™¤")
        return True
    except Exception as e:
        logger.error(f"âŒ æ¸…é™¤ç¾¤ç»„ç¼“å­˜å¤±è´¥: {e}")
        return False


async def get_group_cache_info(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„ç¼“å­˜ä¿¡æ¯"""
    try:
        # è·å–Redisä¸­è¯¥ç¾¤ç»„çš„æ‰€æœ‰ç¼“å­˜é”®
        redis_keys = redis_cache.get_keys(f"group:{group_id}:*")
        redis_keys.extend(redis_cache.get_keys(f"messages:{group_id}:*"))

        # è·å–å†…å­˜ä¸­è¯¥ç¾¤ç»„çš„ç¼“å­˜é”®
        memory_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k or f"messages:{group_id}:" in k]

        # è·å–å¾…å†™å…¥çš„æ¶ˆæ¯æ•°é‡
        pending_count = len(pending_writes.get(group_id, []))

        return {
            "group_id": group_id,
            "redis_cache_keys": len(redis_keys),
            "memory_cache_keys": len(memory_keys),
            "pending_writes": pending_count,
            "redis_connected": redis_cache.is_connected()
        }
    except Exception as e:
        logger.error(f"âŒ è·å–ç¾¤ç»„ {group_id} ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "group_id": group_id,
            "error": str(e)
        }


# ======================= æ€§èƒ½ç›‘æ§ =======================
def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    redis_info = redis_cache.get_info()

    return {
        "memory_cache_size": len(memory_cache),
        "pending_writes_groups": len(pending_writes),
        "pending_writes_total": sum(len(msgs) for msgs in pending_writes.values()),
        "redis_connected": redis_cache.is_connected(),
        "redis_info": redis_info,
        "db_initialized": _db_initialized,
        "periodic_task_running": _periodic_task is not None and not _periodic_task.done()
    }

