import aiosqlite
import json
import asyncio
import time
import os
from collections import defaultdict
from threading import Lock
import hashlib
from developTools.utils.logger import get_logger
from framework_common.database_util.RedisCacheManager import create_group_cache_manager
from run.ai_llm.service.aiReplyHandler.gemini import gemini_prompt_elements_construct
from run.ai_llm.service.aiReplyHandler.openai import prompt_elements_construct, prompt_elements_construct_old_version

# å¯¼å…¥Redisç¼“å­˜ç®¡ç†å™¨


DB_NAME = "data/dataBase/group_messages.db"

# ä¼˜åŒ–åçš„ç¼“å­˜é…ç½®
REDIS_CACHE_TTL = 300  # å¢åŠ åˆ°5åˆ†é’Ÿ
MEMORY_CACHE_TTL = 60  # å†…å­˜ç¼“å­˜1åˆ†é’Ÿ
BATCH_SIZE = 10  # æ‰¹é‡å†™å…¥å¤§å°

logger = get_logger()

# ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨ (æ•°æ®åº“0)
redis_cache = create_group_cache_manager(cache_ttl=REDIS_CACHE_TTL)

# å†…å­˜ç¼“å­˜å’Œæ‰¹é‡å†™å…¥
memory_cache = {}
cache_timestamps = {}
pending_writes = defaultdict(list)
write_lock = Lock()
last_batch_write = time.time()


# ======================= ä¼˜åŒ–çš„ç¼“å­˜ç®¡ç† =======================
def get_cache_key(group_id: int, prompt_standard: str, data_length: int = 20):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"group:{group_id}:{prompt_standard}:{data_length}"


def get_memory_cache(key: str):
    """è·å–å†…å­˜ç¼“å­˜"""
    if key in memory_cache:
        timestamp = cache_timestamps.get(key, 0)
        if time.time() - timestamp < MEMORY_CACHE_TTL:
            return memory_cache[key]
        else:
            # è¿‡æœŸæ¸…ç†
            memory_cache.pop(key, None)
            cache_timestamps.pop(key, None)
    return None


def set_memory_cache(key: str, value):
    """è®¾ç½®å†…å­˜ç¼“å­˜"""
    memory_cache[key] = value
    cache_timestamps[key] = time.time()

    # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    if len(memory_cache) > 1000:
        current_time = time.time()
        expired_keys = [
            k for k, t in cache_timestamps.items()
            if current_time - t > MEMORY_CACHE_TTL
        ]
        for k in expired_keys:
            memory_cache.pop(k, None)
            cache_timestamps.pop(k, None)


def get_redis_cache(key: str):
    """å®‰å…¨è·å–Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    return redis_cache.get(key)


def set_redis_cache(key: str, value, ttl: int = REDIS_CACHE_TTL):
    """å®‰å…¨è®¾ç½®Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    redis_cache.set(key, value, ttl)


def clear_redis_cache_pattern(pattern: str):
    """æ¸…ç†Redisç¼“å­˜æ¨¡å¼ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨"""
    redis_cache.delete_pattern(pattern)


# ======================= ä¼˜åŒ–çš„æ•°æ®åº“æ“ä½œ =======================
MAX_RETRIES = 3
INITIAL_DELAY = 0.1
CONNECTION_POOL = {}


async def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆè¿æ¥æ± ï¼‰"""
    thread_id = id(asyncio.current_task())
    if thread_id not in CONNECTION_POOL:
        db = await aiosqlite.connect(DB_NAME)
        # ä¼˜åŒ–æ•°æ®åº“è®¾ç½®
        await db.execute("PRAGMA journal_mode=WAL;")
        await db.execute("PRAGMA synchronous=NORMAL;")
        await db.execute("PRAGMA cache_size=10000;")
        await db.execute("PRAGMA temp_store=MEMORY;")
        await db.execute("PRAGMA busy_timeout=5000;")
        CONNECTION_POOL[thread_id] = db
    return CONNECTION_POOL[thread_id]


async def execute_with_retry(db, query, params=None):
    """ä¼˜åŒ–çš„å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œ"""
    for attempt in range(MAX_RETRIES):
        try:
            if params:
                cursor = await db.execute(query, params)
            else:
                cursor = await db.execute(query)
            return cursor
        except aiosqlite.OperationalError as e:
            if "database is locked" in str(e) or "busy" in str(e):
                delay = INITIAL_DELAY * (2 ** attempt) + (attempt * 0.05)  # æ›´çŸ­çš„é€€é¿æ—¶é—´
                logger.debug(f"Database busy, retrying in {delay:.3f}s (attempt {attempt + 1})")
                await asyncio.sleep(delay)
            else:
                raise
        except Exception as e:
            logger.error(f"Database error: {e}")
            raise
    raise Exception(f"Database still busy after {MAX_RETRIES} attempts")


# ======================= æ‰¹é‡å†™å…¥ä¼˜åŒ– =======================
async def batch_write_pending():
    """æ‰¹é‡å†™å…¥å¾…å¤„ç†çš„æ•°æ®"""
    global last_batch_write
    current_time = time.time()

    if current_time - last_batch_write < 1.0:  # 1ç§’å†…ä¸é‡å¤å†™å…¥
        return

    with write_lock:
        if not pending_writes:
            return

        batch_data = dict(pending_writes)
        pending_writes.clear()
        last_batch_write = current_time

    if not batch_data:
        return

    try:
        db = await get_db_connection()
        for group_id, messages in batch_data.items():
            if messages:
                # æ‰¹é‡æ’å…¥
                insert_data = [
                    (group_id, json.dumps(msg), None, None, None)
                    for msg in messages
                ]
                await db.executemany(
                    "INSERT INTO group_messages (group_id, message, processed_message, new_openai_processed_message, old_openai_processed_message) VALUES (?, ?, ?, ?, ?)",
                    insert_data
                )

                # æ¸…ç†æ—§æ•°æ®
                cursor = await db.execute("SELECT COUNT(*) FROM group_messages WHERE group_id = ?", (group_id,))
                count = (await cursor.fetchone())[0]

                if count > 50:
                    excess = count - 50
                    await execute_with_retry(
                        db,
                        "DELETE FROM group_messages WHERE id IN (SELECT id FROM group_messages WHERE group_id = ? ORDER BY timestamp ASC LIMIT ?)",
                        (group_id, excess)
                    )

        await db.commit()
        # logger.debug(f"æ‰¹é‡å†™å…¥å®Œæˆ: {len(batch_data)} ä¸ªç¾¤ç»„")

        # æ¸…ç†ç›¸å…³ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        for group_id in batch_data.keys():
            clear_redis_cache_pattern(f"group:{group_id}:*")
            # æ¸…ç†å†…å­˜ç¼“å­˜
            expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k]
            for k in expired_keys:
                memory_cache.pop(k, None)
                cache_timestamps.pop(k, None)

    except Exception as e:
        logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")


# ======================= å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡ç®¡ç† =======================
from typing import Optional

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡å’Œåˆå§‹åŒ–çŠ¶æ€
_periodic_task: Optional[asyncio.Task] = None
_db_initialized: bool = False


async def periodic_batch_write():
    """å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    while True:
        try:
            await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
            await batch_write_pending()
        except Exception as e:
            logger.error(f"å®šæœŸæ‰¹é‡å†™å…¥é”™è¯¯: {e}")


def start_periodic_batch_write():
    """å¯åŠ¨å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            logger.info("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç¨åå†å¯åŠ¨
        logger.debug("æš‚æ—¶æ— æ³•å¯åŠ¨å®šæœŸä»»åŠ¡ï¼Œç­‰å¾…äº‹ä»¶å¾ªç¯å¯åŠ¨")


def stop_periodic_batch_write():
    """åœæ­¢å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡"""
    global _periodic_task
    if _periodic_task and not _periodic_task.done():
        _periodic_task.cancel()
        logger.info("ğŸ›‘ å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²åœæ­¢")


def ensure_periodic_task():
    """ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ"""
    global _periodic_task
    try:
        loop = asyncio.get_running_loop()
        if _periodic_task is None or _periodic_task.done():
            _periodic_task = loop.create_task(periodic_batch_write())
            logger.debug("âœ… å®šæœŸæ‰¹é‡å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¿½ç•¥
        pass


async def ensure_db_initialized():
    """ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–"""
    global _db_initialized
    if not _db_initialized:
        await init_db()
        _db_initialized = True


# ======================= åˆå§‹åŒ– =======================
async def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œæ£€æŸ¥å¹¶æ·»åŠ å¿…è¦çš„å­—æ®µ"""
    db = await get_db_connection()
    try:
        await execute_with_retry(db, """
            CREATE TABLE IF NOT EXISTS group_messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_id INTEGER NOT NULL,
                message TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                processed_message TEXT,
                new_openai_processed_message TEXT,
                old_openai_processed_message TEXT
            )
        """)

        # åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_timestamp ON group_messages(group_id, timestamp);")
        await db.execute("CREATE INDEX IF NOT EXISTS idx_group_id ON group_messages(group_id);")

        await db.commit()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # å¯åŠ¨å®šæœŸä»»åŠ¡
        start_periodic_batch_write()

    except Exception as e:
        logger.warning(f"Error initializing database: {e}")


# ======================= ä¼˜åŒ–çš„æ·»åŠ æ¶ˆæ¯ =======================
async def add_to_group(group_id: int, message, delete_after: int = 50):
    """å‘ç¾¤ç»„æ·»åŠ æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨æ‰¹é‡å†™å…¥ï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # ç¡®ä¿å®šæœŸä»»åŠ¡æ­£åœ¨è¿è¡Œ
    ensure_periodic_task()

    with write_lock:
        pending_writes[group_id].append(message)

        # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿçš„æ¶ˆæ¯ï¼Œç«‹å³å†™å…¥
        if len(pending_writes[group_id]) >= BATCH_SIZE:
            asyncio.create_task(batch_write_pending())


async def get_group_messages(group_id: int, limit: int = 50):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æŒ‡å®šæ•°é‡æ¶ˆæ¯ï¼Œä»…è¿”å›æ–‡æœ¬çš„åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
    cache_key = f"messages:{group_id}:{limit}"
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    try:
        query = "SELECT message FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC"
        params = (group_id,)
        if limit is not None:
            query += " LIMIT ?"
            params += (limit,)

        db = await get_db_connection()
        cursor = await execute_with_retry(db, query, params)
        rows = await cursor.fetchall()

        text_list = []
        for row in rows:
            try:
                raw_message = json.loads(row[0])
                if "message" in raw_message and isinstance(raw_message["message"], list):
                    for msg_obj in raw_message["message"]:
                        if isinstance(msg_obj, dict) and "text" in msg_obj and isinstance(msg_obj["text"], str):
                            text_list.append(msg_obj["text"])
            except (json.JSONDecodeError, KeyError):
                pass

        # ç¼“å­˜ç»“æœ
        set_memory_cache(cache_key, text_list)
        return text_list

    except Exception as e:
        logger.info(f"Error getting messages for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„è·å–å¹¶è½¬æ¢æ¶ˆæ¯ =======================

async def get_last_20_and_convert_to_prompt(group_id: int, data_length=20, prompt_standard="gemini", bot=None,
                                            event=None):
    """è·å–æœ€è¿‘çš„æ¶ˆæ¯å¹¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼çš„ promptï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    cache_key = get_cache_key(group_id, prompt_standard, data_length)

    # ä¸‰çº§ç¼“å­˜ï¼šå†…å­˜ -> Redis -> æ•°æ®åº“
    # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    cached = get_memory_cache(cache_key)
    if cached:
        return cached

    # 2. æ£€æŸ¥Redisç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
    cached = get_redis_cache(cache_key)
    if cached:
        set_memory_cache(cache_key, cached)
        return cached

    # æ˜ å°„ä¸åŒçš„æ ‡å‡†å­—æ®µ
    field_mapping = {
        "gemini": "processed_message",
        "new_openai": "new_openai_processed_message",
        "old_openai": "old_openai_processed_message"
    }

    if prompt_standard not in field_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„ prompt_standard: {prompt_standard}")

    selected_field = field_mapping[prompt_standard]

    # 3. ä»æ•°æ®åº“è·å–
    try:
        # å…ˆç«‹å³å†™å…¥å¾…å¤„ç†çš„æ¶ˆæ¯
        await batch_write_pending()

        db = await get_db_connection()
        cursor = await execute_with_retry(
            db,
            f"SELECT id, message, {selected_field}, timestamp FROM group_messages WHERE group_id = ? ORDER BY timestamp DESC LIMIT ?",
            (group_id, data_length)
        )
        rows = await cursor.fetchall()

        final_list = []
        updates_needed = []  # æ”¶é›†éœ€è¦æ›´æ–°çš„æ•°æ®

        # ç”¨äºæ„å»ºä¸Šä¸‹æ–‡æ‘˜è¦çš„ä¿¡æ¯
        context_info = {
            'participants': set(),
            'message_count': len(rows),
            'topics': [],
            'activities': []
        }

        for i, row in enumerate(rows):
            message_id, raw_message, processed_message, timestamp = row
            raw_message = json.loads(raw_message)

            # æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯
            user_name = raw_message.get('user_name', 'æœªçŸ¥ç”¨æˆ·')
            user_id = raw_message.get('user_id', '')
            context_info['participants'].add(f"{user_name}(ID:{user_id})")

            # åˆ†ææ¶ˆæ¯å†…å®¹ç±»å‹
            message_content = raw_message.get("message", [])
            content_types = []
            for msg_part in message_content:
                if msg_part.get('type') == 'text':
                    text = msg_part.get('text', '').strip()
                    if text:
                        # ç®€å•çš„è¯é¢˜æå–ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
                        if '?' in text or 'ï¼Ÿ' in text:
                            context_info['activities'].append('æœ‰äººæé—®')
                        if any(word in text for word in ['å›¾ç‰‡', 'ç…§ç‰‡', 'çœ‹çœ‹']):
                            context_info['activities'].append('è®¨è®ºå›¾ç‰‡')
                        if any(word in text for word in ['æ–‡ä»¶', 'é“¾æ¥', 'http']):
                            context_info['activities'].append('åˆ†äº«æ–‡ä»¶/é“¾æ¥')
                elif msg_part.get('type') == 'image':
                    content_types.append('å›¾ç‰‡')
                elif msg_part.get('type') == 'file':
                    content_types.append('æ–‡ä»¶')
                elif msg_part.get('type') == 'audio':
                    content_types.append('è¯­éŸ³')
                elif msg_part.get('type') == 'video':
                    content_types.append('è§†é¢‘')

            if content_types:
                context_info['activities'].extend([f"å‘é€äº†{ct}" for ct in content_types])

            # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œä½¿ç”¨ç¼“å­˜çš„æ¶ˆæ¯
            if processed_message:
                final_list.append(json.loads(processed_message))
            else:
                # æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æç¤ºä¿¡æ¯
                position_desc = "æœ€æ–°" if i == 0 else f"ç¬¬{i + 1}æ¡"

                context_prompt = (
                    f"ã€ç¾¤èŠä¸Šä¸‹æ–‡-{position_desc}æ¶ˆæ¯ã€‘"
                    f"å‘é€è€…ï¼š{user_name}(ID:{user_id}) | "
                    f"æ—¶é—´æˆ³ï¼š{timestamp} | "
                    f"æ¶ˆæ¯ä½ç½®ï¼šå€’æ•°ç¬¬{i + 1}æ¡"
                )

                raw_message["message"].insert(0, {
                    "text": f"{context_prompt}\nè¿™æ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ï¼Œç”¨äºç†è§£å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ã€‚å½“æˆ‘å†æ¬¡å‘ä½ æé—®æ—¶ï¼Œè¯·ç»“åˆè¿™äº›ä¸Šä¸‹æ–‡ä¿¡æ¯æ­£å¸¸å›å¤æˆ‘ã€‚"
                })

                if prompt_standard == "gemini":
                    processed = await gemini_prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                elif prompt_standard == "new_openai":
                    processed = await prompt_elements_construct(raw_message["message"], bot=bot, event=event)
                    final_list.append(processed)
                else:
                    processed = await prompt_elements_construct_old_version(raw_message["message"], bot=bot,
                                                                            event=event)
                    final_list.append(processed)

                # æ”¶é›†æ›´æ–°æ•°æ®
                updates_needed.append((json.dumps(processed), message_id, selected_field))

        # æ‰¹é‡æ›´æ–°æ•°æ®åº“
        if updates_needed:
            for processed_json, message_id, field in updates_needed:
                await execute_with_retry(
                    db,
                    f"UPDATE group_messages SET {field} = ? WHERE id = ?",
                    (processed_json, message_id)
                )
            await db.commit()

        # æ„å»ºç¾¤èŠæ¦‚å†µæ‘˜è¦
        participants_list = list(context_info['participants'])
        activities_summary = list(set(context_info['activities'])) if context_info['activities'] else ['æ­£å¸¸èŠå¤©']

        group_summary = (
            f"ã€ç¾¤èŠæ¦‚å†µã€‘å‚ä¸äººæ•°ï¼š{len(participants_list)}äºº | "
            f"æ¶ˆæ¯æ€»æ•°ï¼š{context_info['message_count']}æ¡ | "
            f"ä¸»è¦å‚ä¸è€…ï¼š{', '.join(participants_list[:5])}{'...' if len(participants_list) > 5 else ''} | "
            f"æ´»åŠ¨ç±»å‹ï¼š{', '.join(activities_summary[:3])}{'...' if len(activities_summary) > 3 else ''}"
        )

        # å¤„ç†æœ€ç»ˆæ ¼å¼åŒ–çš„æ¶ˆæ¯
        fl = []
        if prompt_standard == "gemini":
            all_parts = [part for entry in final_list if entry['role'] == 'user' for part in entry['parts']]

            # åœ¨å¼€å¤´æ·»åŠ ç¾¤èŠæ¦‚å†µ
            summary_part = {"text": f"{group_summary}\nä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ä½ ç†è§£å¯¹è¯èƒŒæ™¯ã€‚"}
            all_parts.insert(0, summary_part)

            fl.append({"role": "user", "parts": all_parts})
            fl.append({"role": "model", "parts": {
                "text": "æˆ‘å·²ç»äº†è§£äº†ç¾¤èŠçš„ä¸Šä¸‹æ–‡èƒŒæ™¯ï¼ŒåŒ…æ‹¬å‚ä¸æˆå‘˜ã€æ¶ˆæ¯å†å²å’Œä¸»è¦æ´»åŠ¨ã€‚æˆ‘ä¼šç»“åˆè¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°ç†è§£å’Œå›åº”åç»­çš„å¯¹è¯ã€‚"}})
        else:
            all_parts = []
            all_parts_str = f"{group_summary}\n"

            for entry in final_list:
                if entry['role'] == 'user':
                    if isinstance(entry['content'], str):
                        all_parts_str += entry['content'] + "\n"
                    else:
                        for part in entry['content']:
                            all_parts.append(part)

            if all_parts:
                # åœ¨å¼€å¤´æ·»åŠ æ¦‚å†µè¯´æ˜
                summary_part = {"type": "text", "text": f"{group_summary}\nä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼š"}
                all_parts.insert(0, summary_part)
                fl.append({"role": "user", "content": all_parts})
            else:
                fl.append({"role": "user", "content": all_parts_str + "ä»¥ä¸Šæ˜¯ç¾¤èŠå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ã€‚"})

            fl.append({"role": "assistant",
                       "content": "æˆ‘å·²ç»äº†è§£äº†ç¾¤èŠçš„ä¸Šä¸‹æ–‡èƒŒæ™¯ï¼ŒåŒ…æ‹¬å‚ä¸æˆå‘˜ã€æ¶ˆæ¯å†å²å’Œä¸»è¦æ´»åŠ¨ã€‚æˆ‘ä¼šç»“åˆè¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°ç†è§£å’Œå›åº”åç»­çš„å¯¹è¯ã€‚"})

        # è®¾ç½®ä¸‰çº§ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        set_memory_cache(cache_key, fl)
        set_redis_cache(cache_key, fl)
        print(fl)
        return fl

    except Exception as e:
        logger.info(f"Error getting last 20 and converting to prompt for group {group_id}: {e}")
        return []


# ======================= ä¼˜åŒ–çš„æ¸…é™¤æ¶ˆæ¯ =======================
async def clear_group_messages(group_id: int):
    """æ¸…é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
    await ensure_db_initialized()

    try:
        # å…ˆæ¸…ç†å¾…å†™å…¥çš„æ•°æ®
        with write_lock:
            pending_writes.pop(group_id, None)

        db = await get_db_connection()
        await execute_with_retry(
            db,
            "DELETE FROM group_messages WHERE group_id = ?",
            (group_id,)
        )
        await db.commit()
        logger.info(f"âœ… å·²æ¸…é™¤ group_id={group_id} çš„æ‰€æœ‰æ•°æ®")

        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜ - ä½¿ç”¨Redisç¼“å­˜ç®¡ç†å™¨
        clear_redis_cache_pattern(f"group:{group_id}:*")

        # æ¸…ç†å†…å­˜ç¼“å­˜
        expired_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k or f"messages:{group_id}:" in k]
        for k in expired_keys:
            memory_cache.pop(k, None)
            cache_timestamps.pop(k, None)

    except Exception as e:
        logger.error(f"âŒ æ¸…ç† group_id={group_id} æ•°æ®æ—¶å‡ºé”™: {e}")


# ======================= æ–°å¢ï¼šç¼“å­˜ç®¡ç†åŠŸèƒ½ =======================
async def clear_all_group_cache():
    """æ¸…é™¤æ‰€æœ‰ç¾¤ç»„ç›¸å…³çš„ç¼“å­˜"""
    try:
        # æ¸…é™¤Redisç¼“å­˜
        redis_cache.delete_pattern("group:*")
        redis_cache.delete_pattern("messages:*")

        # æ¸…é™¤å†…å­˜ç¼“å­˜
        memory_cache.clear()
        cache_timestamps.clear()

        logger.info("âœ… æ‰€æœ‰ç¾¤ç»„ç¼“å­˜å·²æ¸…é™¤")
        return True
    except Exception as e:
        logger.error(f"âŒ æ¸…é™¤ç¾¤ç»„ç¼“å­˜å¤±è´¥: {e}")
        return False


async def get_group_cache_info(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„ç¼“å­˜ä¿¡æ¯"""
    try:
        # è·å–Redisä¸­è¯¥ç¾¤ç»„çš„æ‰€æœ‰ç¼“å­˜é”®
        redis_keys = redis_cache.get_keys(f"group:{group_id}:*")
        redis_keys.extend(redis_cache.get_keys(f"messages:{group_id}:*"))

        # è·å–å†…å­˜ä¸­è¯¥ç¾¤ç»„çš„ç¼“å­˜é”®
        memory_keys = [k for k in memory_cache.keys() if f"group:{group_id}:" in k or f"messages:{group_id}:" in k]

        # è·å–å¾…å†™å…¥çš„æ¶ˆæ¯æ•°é‡
        pending_count = len(pending_writes.get(group_id, []))

        return {
            "group_id": group_id,
            "redis_cache_keys": len(redis_keys),
            "memory_cache_keys": len(memory_keys),
            "pending_writes": pending_count,
            "redis_connected": redis_cache.is_connected()
        }
    except Exception as e:
        logger.error(f"âŒ è·å–ç¾¤ç»„ {group_id} ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "group_id": group_id,
            "error": str(e)
        }


# ======================= æ€§èƒ½ç›‘æ§ =======================
def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    redis_info = redis_cache.get_info()

    return {
        "memory_cache_size": len(memory_cache),
        "pending_writes_groups": len(pending_writes),
        "pending_writes_total": sum(len(msgs) for msgs in pending_writes.values()),
        "redis_connected": redis_cache.is_connected(),
        "redis_info": redis_info,
        "db_initialized": _db_initialized,
        "periodic_task_running": _periodic_task is not None and not _periodic_task.done()
    }


# ======================= æ–°å¢ï¼šæ‰‹åŠ¨ç¼“å­˜æ§åˆ¶ =======================
def force_cache_cleanup():
    """å¼ºåˆ¶æ¸…ç†è¿‡æœŸçš„å†…å­˜ç¼“å­˜"""
    try:
        current_time = time.time()
        expired_keys = [
            k for k, t in cache_timestamps.items()
            if current_time - t > MEMORY_CACHE_TTL
        ]

        for k in expired_keys:
            memory_cache.pop(k, None)
            cache_timestamps.pop(k, None)

        logger.info(f"âœ… æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸçš„å†…å­˜ç¼“å­˜é¡¹")
        return len(expired_keys)
    except Exception as e:
        logger.error(f"âŒ å¼ºåˆ¶æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
        return 0


async def preload_group_cache(group_id: int, data_length: int = 20):
    """é¢„åŠ è½½ç¾¤ç»„ç¼“å­˜"""
    try:
        # é¢„åŠ è½½ä¸åŒpromptæ ‡å‡†çš„ç¼“å­˜
        standards = ["gemini", "new_openai", "old_openai"]

        for standard in standards:
            await get_last_20_and_convert_to_prompt(
                group_id=group_id,
                data_length=data_length,
                prompt_standard=standard
            )

        # é¢„åŠ è½½æ¶ˆæ¯åˆ—è¡¨ç¼“å­˜
        await get_group_messages(group_id, limit=50)

        logger.info(f"âœ… ç¾¤ç»„ {group_id} ç¼“å­˜é¢„åŠ è½½å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ ç¾¤ç»„ {group_id} ç¼“å­˜é¢„åŠ è½½å¤±è´¥: {e}")
        return False


# ======================= å…³é—­èµ„æº =======================
async def cleanup_resources():
    """æ¸…ç†èµ„æº"""
    try:
        # åœæ­¢å®šæœŸä»»åŠ¡
        stop_periodic_batch_write()

        # æœ€åä¸€æ¬¡æ‰¹é‡å†™å…¥
        await batch_write_pending()

        # å…³é—­æ•°æ®åº“è¿æ¥
        for db in CONNECTION_POOL.values():
            await db.close()
        CONNECTION_POOL.clear()

        # æ¸…ç†å†…å­˜ç¼“å­˜
        memory_cache.clear()
        cache_timestamps.clear()
        pending_writes.clear()

        logger.info("âœ… ç¾¤ç»„æ¶ˆæ¯æ¨¡å—èµ„æºæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")