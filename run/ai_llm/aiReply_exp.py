
import asyncio
import base64
import datetime
import io
import os
import re
import time
import traceback
from typing import Dict

import httpx
from PIL import Image
from dataclasses import dataclass, field

from developTools.event.events import GroupMessageEvent, LifecycleMetaEvent
from developTools.message.message_components import At
from framework_common.database_util.Group import get_last_20_and_convert_to_prompt
from framework_common.database_util.User import get_user, update_user
from framework_common.database_util.llmDB import delete_latest2_history, read_chara, use_folder_chara
from run.ai_llm.service.aiReplyCore import aiReplyCore, send_text, count_tokens_approximate
from run.ai_llm.service.heartflow_client import heartflow_request
from run.ai_llm.service.schemaReplyCore import schemaReplyCore

# ç”¨äºåŒ¹é… base64 æ•°æ®URIçš„æ­£åˆ™
BASE64_PATTERN = re.compile(r'^data:([^;]+);base64,(.+)$', re.DOTALL)


def is_local_file_path(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„"""
    if url.startswith("file://"):
        return True
    if len(url) >= 2 and url[1] == ':' and url[0].isalpha():
        return True
    if url.startswith("/") and not url.startswith("//"):
        return True
    return False


def get_local_file_path(url: str) -> str:
    """è·å–æœ¬åœ°æ–‡ä»¶çš„å®é™…è·¯å¾„"""
    if url.startswith("file://"):
        return url[7:]
    return url


async def _process_image_for_heartflow(url: str, client_type: str) -> dict:
    try:
        img_base64 = None
        base64_match = BASE64_PATTERN.match(url)
        if base64_match:
            img_base64 = base64_match.group(2)
        elif is_local_file_path(url):
            actual_path = get_local_file_path(url)
            if os.path.exists(actual_path):
                image = Image.open(actual_path)
                image = image.convert("RGB")
                img_byte_arr = io.BytesIO()
                quality = 85
                while True:
                    img_byte_arr.seek(0)
                    img_byte_arr.truncate()
                    image.save(img_byte_arr, format='JPEG', quality=quality)
                    if img_byte_arr.tell() / 1024 <= 400 or quality <= 10:
                        break
                    quality -= 5
                img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
                image.close()
        else:
            async with httpx.AsyncClient(timeout=30) as client:
                res = await client.get(url)
                if res.status_code == 200:
                    image = Image.open(io.BytesIO(res.content))
                    image = image.convert("RGB")
                    img_byte_arr = io.BytesIO()
                    quality = 85
                    while True:
                        img_byte_arr.seek(0)
                        img_byte_arr.truncate()
                        image.save(img_byte_arr, format='JPEG', quality=quality)
                        if img_byte_arr.tell() / 1024 <= 400 or quality <= 10:
                            break
                        quality -= 5
                    img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
                    image.close()
        
        if not img_base64:
            return None
        if client_type == "openai":
            return {"input_image": {"image_url": f"data:image/jpeg;base64,{img_base64}", "detail": "auto"}}
        else:
            return {"inlineData": {"mimeType": "image/jpeg", "data": img_base64}}
    
    except Exception as e:
        print(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
        return None


@dataclass
class JudgeResult:
    """åˆ¤æ–­ç»“æœæ•°æ®ç±»"""
    relevance: float = 0.0
    willingness: float = 0.0
    social: float = 0.0
    timing: float = 0.0
    continuity: float = 0.0
    reasoning: str = ""
    should_reply: bool = False
    confidence: float = 0.0
    overall_score: float = 0.0


@dataclass
class ChatState:
    """ç¾¤èŠçŠ¶æ€æ•°æ®ç±»"""
    energy: float = 1.0
    last_reply_time: float = 0.0
    last_reset_date: str = ""
    total_messages: int = 0
    total_replies: int = 0
    recent_interactions: Dict[int, float] = field(default_factory=dict)


async def heartflow_reply(config, prompt, group_messages_bg=None, recursion_times=0, image_parts=None):
    try:
        messages = [{"text": prompt}]
        if image_parts:
            messages.extend(image_parts)
        
        result = await heartflow_request(
            config,
            messages,
            system_instruction=None,
            group_context=group_messages_bg,
        )
        
        if result:
            print(result)
        return result
        
    except Exception as e:
        traceback.print_exc()
        recursion_times += 1
        print(f"Recursion times: {recursion_times}")
        recursion_limit = config.ai_llm.config["llm"].get("retries", 3)
        if recursion_times > recursion_limit:
            return None
        return await heartflow_reply(config, prompt, group_messages_bg, recursion_times, image_parts)
def main(bot, config):
    """
    æ­¤æ’ä»¶ä»£ç å‚è€ƒäº†https://github.com/advent259141/Astrbot_plugin_Heartflow
    """
    """å¿ƒæµæ’ä»¶ä¸»å‡½æ•°"""
    summarized_chara=None
    # è·å–toolsé…ç½®ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰
    tools = None
    if config.ai_llm.config["llm"]["func_calling"]:
        from framework_common.framework_util.func_map_loader import build_tool_map
        tools = build_tool_map()

    # ============ é…ç½®è¯»å– ============



    # åˆ¤æ–­æƒé‡é…ç½®
    weights = {
        "relevance": config.ai_llm.config["heartflow"]["weight_relevance"],
        "willingness": config.ai_llm.config["heartflow"]["weight_willingness"],
        "social": config.ai_llm.config["heartflow"]["weight_social"],
        "timing": config.ai_llm.config["heartflow"]["weight_timing"],
        "continuity": config.ai_llm.config["heartflow"]["weight_continuity"],
    }

    # å½’ä¸€åŒ–æƒé‡
    weight_sum = sum(weights.values())
    if abs(weight_sum - 1.0) > 1e-6:
        bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šåˆ¤æ–­æƒé‡å’Œä¸ä¸º1 ({weight_sum})ï¼Œå·²è‡ªåŠ¨å½’ä¸€åŒ–")
        weights = {k: v / weight_sum for k, v in weights.items()}

    # ============ çŠ¶æ€ç®¡ç† ============
    chat_states: Dict[int, ChatState] = {}
    persona_cache: Dict[str, str] = {}
    user_state = {}  # ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—çŠ¶æ€
    portrait_updating = set()  # æ­£åœ¨æ›´æ–°ç”»åƒçš„ç”¨æˆ·

    # ============ å·¥å…·å‡½æ•° ============

    def get_chat_state(group_id: int) -> ChatState:
        """è·å–ç¾¤èŠçŠ¶æ€"""
        if group_id not in chat_states:
            chat_states[group_id] = ChatState()

        state = chat_states[group_id]
        today = datetime.date.today().isoformat()
        if state.last_reset_date != today:
            state.last_reset_date = today
            state.energy = min(1.0, state.energy + 0.2)
            bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šç¾¤ {group_id} æ¯æ—¥é‡ç½®ï¼Œç²¾åŠ›æ¢å¤è‡³ {state.energy:.2f}")

        return state

    def get_minutes_since_last_reply(group_id: int) -> int:
        """è·å–è·ç¦»ä¸Šæ¬¡å›å¤çš„åˆ†é’Ÿæ•°"""
        state = get_chat_state(group_id)
        if state.last_reply_time == 0:
            return 999
        return int((time.time() - state.last_reply_time) / 60)

    def update_active_state(group_id: int, user_id: int):
        """æ›´æ–°ä¸»åŠ¨å›å¤çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.last_reply_time = time.time()
        state.total_replies += 1
        state.total_messages += 1
        state.energy = max(0.1, state.energy - config.ai_llm.config["heartflow"]["energy_decay_rate"])
        state.recent_interactions[user_id] = time.time()
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°ä¸»åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def update_passive_state(group_id: int):
        """æ›´æ–°è¢«åŠ¨çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.total_messages += 1
        state.energy = min(1.0, state.energy + config.ai_llm.config["heartflow"]["energy_recovery_rate"])
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°è¢«åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def check_recent_interaction(group_id: int, user_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„äº¤äº’è®°å½•"""
        state = get_chat_state(group_id)
        if user_id not in state.recent_interactions:
            return False

        last_time = state.recent_interactions[user_id]
        time_diff = time.time() - last_time

        if time_diff > config.ai_llm.config["heartflow"]["interaction_timeout"]:
            del state.recent_interactions[user_id]
            return False

        return True

    async def get_persona_prompt(user_id: int) -> str:
        """è·å–ç”¨æˆ·çš„äººæ ¼è®¾å®š"""
        try:
            cache_key = f"persona_{user_id}"
            if cache_key in persona_cache:
                return persona_cache[cache_key]

            user_info = await get_user(user_id)
            chara_file = getattr(user_info, 'chara_file', None)

            if not chara_file or chara_file == "default":
                chara_file = config.ai_llm.config["llm"]["chara_file_name"]

            chara_path = f"./data/system/chara/{chara_file}"
            try:

                persona = await read_chara(user_id, await use_folder_chara(config.ai_llm.config["llm"]["chara_file_name"]))
                persona=persona.replace("{bot_name}",config.common_config.basic_config["bot"])
                if len(persona) > 500:
                    persona = await summarize_persona(persona)

                persona_cache[cache_key] = persona
                return persona
            except FileNotFoundError:
                bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šæœªæ‰¾åˆ°è§’è‰²æ–‡ä»¶ {chara_path}")
                return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šè·å–äººæ ¼è®¾å®šå¤±è´¥ {e}")
            return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"

    async def summarize_persona(original_persona: str) -> str:
        """ç²¾ç®€äººæ ¼è®¾å®š"""
        try:
            nonlocal summarized_chara
            if summarized_chara:
                return summarized_chara
            prompt = f"""è¯·å°†ä»¥ä¸‹æœºå™¨äººè§’è‰²è®¾å®šæ€»ç»“ä¸ºç®€æ´çš„æ ¸å¿ƒè¦ç‚¹ã€‚
            æ€»ç»“åçš„å†…å®¹åº”è¯¥åœ¨100-200å­—ä»¥å†…ï¼Œçªå‡ºæœ€é‡è¦çš„è§’è‰²ç‰¹ç‚¹ã€‚
            
            åŸå§‹è§’è‰²è®¾å®šï¼š
            {original_persona}"""

            result = await heartflow_reply(
                config,
                prompt,
                recursion_times=7
            )
            summarized_chara=result
            summarized = result
            if summarized and len(summarized.strip()) > 10:
                bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šäººæ ¼ç²¾ç®€å®Œæˆ {len(original_persona)} -> {len(summarized)}")
                return summarized

            return original_persona
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šç²¾ç®€äººæ ¼å¤±è´¥ {e}")
            return original_persona

    async def judge_should_reply(event: GroupMessageEvent) -> JudgeResult:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å›å¤"""
        try:
            chat_state = get_chat_state(event.group_id)
            persona = await get_persona_prompt(event.user_id)

            heartflow_config = config.ai_llm.config.get("heartflow", {})
            client_config = heartflow_config.get("client", {})
            client_type = client_config.get("type", "gemini").strip().lower()
            listen_image = heartflow_config.get("listen_image", False)
            
            if client_type == "openai":
                prompt_format = "new_openai"
            else:
                prompt_format = "gemini"
            
            # æ ¹æ® listen_image é…ç½®å†³å®šæ˜¯å¦åœ¨ç¾¤èŠä¸Šä¸‹æ–‡ä¸­åŒ…å«å›¾ç‰‡
            group_messages_bg = await get_last_20_and_convert_to_prompt(
                event.group_id, config.ai_llm.config["heartflow"]["context_messages_count"], prompt_format, bot,
                include_images=listen_image
            )

            # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨äº†listen_imageï¼‰
            image_parts = []
            if listen_image and hasattr(event, 'processed_message'):
                for item in event.processed_message:
                    if "image" in item or "mface" in item:
                        try:
                            if "mface" in item:
                                url = item["mface"].get("url") or item["mface"].get("file")
                            else:
                                url = item["image"].get("url") or item["image"].get("file")
                            
                            if url:
                                img_data = await _process_image_for_heartflow(url, client_type)
                                if img_data:
                                    image_parts.append(img_data)
                        except Exception as e:
                            bot.logger.warning(f"å¿ƒæµæ’ä»¶: å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")

            def extract_text_from_message(msg):
                """ä»Gemini/OpenAIæ ¼å¼çš„æ¶ˆæ¯ä¸­æå–æ–‡æœ¬"""
                role = msg.get("role", "")
                if role not in ["user", "model", "assistant"]:
                    return None
                
                # Gemini æ ¼å¼: {"role": "user", "parts": [{"text": "..."}, ...]}
                if "parts" in msg:
                    texts = []
                    for part in msg["parts"]:
                        if isinstance(part, dict) and "text" in part:
                            texts.append(part["text"])
                    return "\n".join(texts) if texts else None
                
                # OpenAI æ ¼å¼: {"role": "user", "content": [{"type": "text", "text": "..."}, ...]}
                if "content" in msg:
                    content = msg["content"]
                    if isinstance(content, str):
                        return content
                    elif isinstance(content, list):
                        texts = []
                        for item in content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                texts.append(item.get("text", ""))
                        return "\n".join(texts) if texts else None
                
                return None
            
            if group_messages_bg:
                recent_texts = []
                for msg in group_messages_bg[-5:]:
                    text = extract_text_from_message(msg)
                    if text:
                        recent_texts.append(text)
                recent_messages = "\n---\n".join(recent_texts) if recent_texts else "æš‚æ— å¯¹è¯å†å²"
            else:
                recent_messages = "æš‚æ— å¯¹è¯å†å²"
            
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            message_content_desc = event.pure_text if event.pure_text else "(æ— æ–‡å­—å†…å®¹)"
            if image_parts:
                message_content_desc += f"\n(é™„å¸¦{len(image_parts)}å¼ å›¾ç‰‡ï¼Œè§ä¸‹æ–¹)"
            image_prefix_text = ""
            if image_parts and not event.pure_text:
                image_prefix_text = f"ä»¥ä¸‹æ˜¯ç”¨æˆ·{event.sender.nickname}å‘é€çš„å›¾ç‰‡:\n"
            
            prompt = f"""ä½ æ˜¯ç¾¤èŠæœºå™¨äººçš„å†³ç­–ç³»ç»Ÿï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨å›å¤ã€‚

                ## æœºå™¨äººè§’è‰²è®¾å®š
                {persona}
                
                ## å½“å‰ç¾¤èŠæƒ…å†µ
                - ç¾¤èŠID: {event.group_id}
                - ç²¾åŠ›æ°´å¹³: {chat_state.energy:.1f}/1.0
                - ä¸Šæ¬¡å‘è¨€: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰
                - å›å¤ç‡: {(chat_state.total_replies / max(1, chat_state.total_messages) * 100):.1f}%
                
                ## æœ€è¿‘å¯¹è¯
                {recent_messages}
                
                ## å¾…åˆ¤æ–­æ¶ˆæ¯
                å‘é€è€…: {event.sender.nickname}
                å†…å®¹: {message_content_desc}
                æ—¶é—´: {datetime.datetime.now().strftime('%H:%M:%S')}
                
                å›å¤é˜ˆå€¼: {reply_threshold}
                è¯·ä»5ä¸ªç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ã€‚
                
                {image_prefix_text}"""
            prompt += """

            è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯åšå‡ºåˆ¤æ–­ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

            ç›¸å…³åº¦: 0-10
            æ„æ„¿: 0-10
            ç¤¾äº¤: 0-10
            æ—¶æœº: 0-10
            è¿è´¯: 0-10
            ç†ç”±: è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆåº”æˆ–ä¸åº”å›å¤ï¼ˆç»“åˆè§’è‰²ç‰¹æ€§ï¼‰

            âš ï¸ è¯·ä¸¥æ ¼ä¿æŒè¯¥æ ¼å¼ï¼Œæ¯ä¸ªåˆ†æ•°å­—åªèƒ½å†™ä¸€ä¸ªçº¯æ•°å­—ã€‚
            """

            result_text = await heartflow_reply(
                config,
                prompt,
                group_messages_bg=group_messages_bg,
                image_parts=image_parts if image_parts else None
            )
            #print(result_text)
            #print(type(result_text))
            # ä½¿ç”¨æ­£åˆ™è§£æåˆ†æ•°
            import re

            def ext(name):
                m = re.search(rf"(?:{name})\s*[:ï¼š]\s*(\d+)", result_text)
                return float(m.group(1)) if m else 0.0

            relevance = ext("ç›¸å…³åº¦|å†…å®¹ç›¸å…³åº¦|relevance")
            willingness = ext("æ„æ„¿|å›å¤æ„æ„¿|willingness")
            social = ext("ç¤¾äº¤|ç¤¾äº¤é€‚å®œæ€§|social")
            timing = ext("æ—¶æœº|æ—¶æœºæ°å½“æ€§|timing")
            continuity = ext("è¿è´¯|å¯¹è¯è¿è´¯|continuity")


            # æå–ç†ç”±
            reasoning_match = re.search(r"(ç†ç”±|åˆ†æ|åŸå› )[:ï¼š]\s*(.+)", result_text, re.S)
            reasoning = reasoning_match.group(2).strip() if reasoning_match else result_text.strip()

            overall_score = (
                                    relevance * weights["relevance"] +
                                    willingness * weights["willingness"] +
                                    social * weights["social"] +
                                    timing * weights["timing"] +
                                    continuity * weights["continuity"]
                            ) / 10.0

            should_reply = overall_score >= reply_threshold
            #print(should_reply)
            r=JudgeResult(
                relevance=relevance,
                willingness=willingness,
                social=social,
                timing=timing,
                continuity=continuity,
                reasoning=reasoning,
                should_reply=should_reply,
                confidence=overall_score,
                overall_score=overall_score
            )
            print(r)
            return r

        except Exception as e:
            traceback.print_exc()
            bot.logger.error(f"å¿ƒæµåˆ¤æ–­å¼‚å¸¸: {e}")
            return JudgeResult(should_reply=False, reasoning=f"å¼‚å¸¸: {str(e)}")

    # ============ æ¶ˆæ¯å¤„ç†é€»è¾‘ï¼ˆå¤åˆ¶è‡ªåŸæ¡†æ¶ï¼‰============

    async def handle_message(event: GroupMessageEvent, user_info=None):
        """å¤„ç†æ¶ˆæ¯çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰"""
        uid = event.user_id
        if user_info is None:
            user_info = await get_user(event.user_id, event.sender.nickname)

        if uid not in user_state:
            user_state[uid] = {
                "queue": asyncio.Queue(),
                "running": False
            }

        await user_state[uid]["queue"].put(event)

        if user_state[uid]["running"]:
            bot.logger.info(f"ç”¨æˆ·{uid}æ­£åœ¨å¤„ç†ä¸­ï¼Œå·²æ”¾å…¥é˜Ÿåˆ—")
            return

        async def process_user_queue(uid):
            user_state[uid]["running"] = True
            try:
                current_event = await user_state[uid]["queue"].get()
                try:
                    current_event.processed_message.append({"text": "(ç³»ç»Ÿæç¤ºï¼šä½ ç›®å‰æ­£å¤„äºç¾¤èŠç¯å¢ƒä¸­ï¼Œè¯·æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡åšå‡ºè‡ªç„¶ã€é•¿åº¦é€‚å½“çš„å›å¤ä»¥èå…¥èŠå¤©ã€‚ä¸è¦è®©æ­¤æç¤ºä¿¡æ¯å‡ºç°åœ¨å›å¤ä¸­ã€‚)"})
                    reply_message = await aiReplyCore(
                        current_event.processed_message,
                        current_event.user_id,
                        config,
                        tools=tools,
                        bot=bot,
                        event=current_event
                    )

                    if reply_message is None or '' == str(reply_message) or 'Maximum recursion depth' in reply_message:
                        return

                    if "call_send_mface(summary='')" in reply_message:
                        reply_message = reply_message.replace("call_send_mface(summary='')", '')

                    try:
                        tokens_total = count_tokens_approximate(
                            current_event.processed_message[1]['text'],
                            reply_message, user_info.ai_token_record
                        )
                        await update_user(user_id=current_event.user_id, ai_token_record=tokens_total)
                    except:
                        pass

                    await send_text(bot, current_event, config, reply_message.strip())

                except Exception as e:
                    bot.logger.exception(f"ç”¨æˆ· {uid} å¤„ç†å‡ºé”™: {e}")
                finally:
                    user_state[uid]["queue"].task_done()

                    if not user_state[uid]["queue"].empty():
                        asyncio.create_task(process_user_queue(uid))
            finally:
                user_state[uid]["running"] = False

        asyncio.create_task(process_user_queue(uid))

    # ============ äº‹ä»¶å¤„ç†å™¨ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_handler(event: GroupMessageEvent):
        """å¿ƒæµä¸»åŠ¨å›å¤å¤„ç†"""

        # è·³è¿‡å‘½ä»¤å’Œbotè‡ªå·±çš„æ¶ˆæ¯
        if event.pure_text and (event.pure_text.startswith("/") or event.pure_text.startswith("#")):
            return
        if event.user_id == bot.id:
            return

        listen_image = config.ai_llm.config.get("heartflow", {}).get("listen_image", False)
        has_text = event.pure_text and event.pure_text.strip()
        has_image = False
        if listen_image and hasattr(event, 'processed_message'):
            for item in event.processed_message:
                if "image" in item or "mface" in item:
                    has_image = True
                    break

        if not has_text and not has_image:
            return
        
        if event.message_chain.has(At):
            if event.message_chain.get(At)[0].qq in [bot.id, 1000000]:
                bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šè·³è¿‡@æœºå™¨äººæ¶ˆæ¯")
                return
        # ç™½åå•æ£€æŸ¥
        if config.ai_llm.config["heartflow"]["whitelist_enabled"]:
            if event.group_id not in config.ai_llm.config["heartflow"]["chat_whitelist"]:
                return

        # å¿ƒæµåˆ¤æ–­
        if config.ai_llm.config["heartflow"]["enabled"]:
            try:
                judge_result = await judge_should_reply(event)

                if judge_result.should_reply:
                    bot.logger.info(
                        f"ğŸ”¥ å¿ƒæµè§¦å‘ | ç¾¤:{event.group_id} | è¯„åˆ†:{judge_result.overall_score:.2f}"
                    )

                    # æƒé™æ£€æŸ¥
                    user_info = await get_user(event.user_id, event.sender.nickname)
                    if not user_info.permission >= config.ai_llm.config["core"]["ai_reply_group"]:
                        return

                    #if event.group_id in [913122269, 1050663831] and not user_info.permission >= 66:
                        #return

                    if not user_info.permission >= config.ai_llm.config["core"]["ai_token_limt"]:
                        if user_info.ai_token_record >= config.ai_llm.config["core"]["ai_token_limt_token"]:
                            return

                    # æ›´æ–°çŠ¶æ€å¹¶å¤„ç†æ¶ˆæ¯
                    update_active_state(event.group_id, event.user_id)
                    await handle_message(event, user_info)
                    return
                else:
                    update_passive_state(event.group_id)

            except Exception as e:
                bot.logger.error(f"å¿ƒæµå¤„ç†å¼‚å¸¸: {e}")



    # ============ ç®¡ç†å‘½ä»¤ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_commands(event: GroupMessageEvent):
        """å¿ƒæµç®¡ç†å‘½ä»¤"""
        if not event.pure_text:
            return

        if event.pure_text == "/heartflow":
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            whitelist_enabled = config.ai_llm.config['heartflow']['whitelist_enabled']
            enabled = config.ai_llm.config['heartflow']['enabled']
            state = get_chat_state(event.group_id)
            status = f"""ğŸ”® å¿ƒæµçŠ¶æ€æŠ¥å‘Š

ğŸ“Š **å½“å‰çŠ¶æ€**
- ç¾¤èŠID: {event.group_id}
- ç²¾åŠ›æ°´å¹³: {state.energy:.2f}/1.0 {'ğŸŸ¢' if state.energy > 0.7 else 'ğŸŸ¡' if state.energy > 0.3 else 'ğŸ”´'}
- ä¸Šæ¬¡å›å¤: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰

ğŸ“ˆ **å†å²ç»Ÿè®¡**
- æ€»æ¶ˆæ¯æ•°: {state.total_messages}
- æ€»å›å¤æ•°: {state.total_replies}
- å›å¤ç‡: {(state.total_replies / max(1, state.total_messages) * 100):.1f}%
- æ´»è·ƒç”¨æˆ·: {len(state.recent_interactions)}äºº

âš™ï¸ **é…ç½®**
- å›å¤é˜ˆå€¼: {reply_threshold}
- ç™½åå•: {'âœ…' if whitelist_enabled else 'âŒ'}
- çŠ¶æ€: {'âœ… å¯ç”¨' if enabled else 'âŒ ç¦ç”¨'}

ğŸ¯ **æƒé‡**
- ç›¸å…³åº¦: {weights['relevance']:.0%}
- æ„æ„¿: {weights['willingness']:.0%}
- ç¤¾äº¤: {weights['social']:.0%}
- æ—¶æœº: {weights['timing']:.0%}
- è¿è´¯: {weights['continuity']:.0%}"""
            await bot.send(event, status)

        elif event.pure_text == "/heartflow_reset":
            if event.group_id in chat_states:
                del chat_states[event.group_id]
            await bot.send(event, "âœ… å¿ƒæµçŠ¶æ€å·²é‡ç½®")

        elif event.pure_text == "/heartflow_cache":
            info = f"ğŸ§  äººæ ¼ç¼“å­˜: {len(persona_cache)}ä¸ª\n\n"
            if persona_cache:
                for key, value in list(persona_cache.items())[:5]:
                    info += f"ğŸ”‘ {key}\nğŸ“„ {value[:80]}...\n\n"
            else:
                info += "ğŸ“­ æ— ç¼“å­˜"
            await bot.send(event, info)

        elif event.pure_text == "/heartflow_cache_clear":
            count = len(persona_cache)
            persona_cache.clear()
            await bot.send(event, f"âœ… å·²æ¸…é™¤ {count} ä¸ªç¼“å­˜")

    bot.logger.info("å¿ƒæµæ’ä»¶å·²åŠ è½½")


