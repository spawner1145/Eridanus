
import asyncio
import datetime
import time
import traceback
from typing import Dict
from dataclasses import dataclass, field

from developTools.event.events import GroupMessageEvent, LifecycleMetaEvent
from framework_common.database_util.Group import get_last_20_and_convert_to_prompt
from framework_common.database_util.User import get_user, update_user
from framework_common.database_util.llmDB import delete_latest2_history, read_chara, use_folder_chara
from framework_common.utils.GeminiKeyManager import GeminiKeyManager
from run.ai_llm.service.aiReplyCore import aiReplyCore, send_text, count_tokens_approximate
from run.ai_llm.service.aiReplyHandler.gemini import geminiRequest
from run.ai_llm.service.schemaReplyCore import schemaReplyCore


@dataclass
class JudgeResult:
    """åˆ¤æ–­ç»“æœæ•°æ®ç±»"""
    relevance: float = 0.0
    willingness: float = 0.0
    social: float = 0.0
    timing: float = 0.0
    continuity: float = 0.0
    reasoning: str = ""
    should_reply: bool = False
    confidence: float = 0.0
    overall_score: float = 0.0


@dataclass
class ChatState:
    """ç¾¤èŠçŠ¶æ€æ•°æ®ç±»"""
    energy: float = 1.0
    last_reply_time: float = 0.0
    last_reset_date: str = ""
    total_messages: int = 0
    total_replies: int = 0
    recent_interactions: Dict[int, float] = field(default_factory=dict)

async def gemma_reply(config,prompt,group_messages_bg=None,recursion_times=0):
    copy_history = []
    copy_history.append({"role": "user", "parts": [{"text": prompt}]})
    if group_messages_bg:
        copy_history.insert(0, group_messages_bg[0])
        copy_history.insert(1, group_messages_bg[1])
    #print(copy_history)
    try:
        response_message = await geminiRequest(
            copy_history,
            config.ai_llm.config["llm"]["gemini"]["base_url"],
            await GeminiKeyManager.get_gemini_apikey(),
            config.ai_llm.config["llm"]["gemini"]["model"],
            config.common_config.basic_config["proxy"]["http_proxy"] if config.ai_llm.config["llm"][
                "enable_proxy"] else None,
            tools=None,
            system_instruction=None,
            temperature=config.ai_llm.config["llm"]["gemini"]["temperature"],
            maxOutputTokens=config.ai_llm.config["llm"]["gemini"]["maxOutputTokens"],
            fallback_models=["gemma-3-27b-it"],
        )
        print(response_message)
        return response_message['candidates'][0]["content"]["parts"][0]["text"]
    except Exception as e:
        traceback.print_exc()
        recursion_times+=1
        print(f"Recursion times: {recursion_times}")
        if recursion_times > config.ai_llm.config["llm"]["recursion_limit"]:
            return None
def main(bot, config):
    """
    æ­¤æ’ä»¶ä»£ç å‚è€ƒäº†https://github.com/advent259141/Astrbot_plugin_Heartflow
    """
    """å¿ƒæµæ’ä»¶ä¸»å‡½æ•°"""
    # è·å–toolsé…ç½®ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰
    tools = None
    if config.ai_llm.config["llm"]["func_calling"]:
        from framework_common.framework_util.func_map_loader import gemini_func_map, openai_func_map
        if config.ai_llm.config["llm"]["model"] == "gemini":
            tools = gemini_func_map()
        else:
            tools = openai_func_map()

    if config.ai_llm.config["llm"]["è”ç½‘æœç´¢"]:
        if config.ai_llm.config["llm"]["model"] == "gemini":
            if tools is None:
                tools = [{"googleSearch": {}}]
            else:
                tools = [{"googleSearch": {}}, tools]
        else:
            if tools is None:
                tools = [{"type": "function", "function": {"name": "googleSearch"}}]
            else:
                tools = [{"type": "function", "function": {"name": "googleSearch"}}, tools]
    # ============ é…ç½®è¯»å– ============



    # åˆ¤æ–­æƒé‡é…ç½®
    weights = {
        "relevance": config.ai_llm.config["heartflow"]["weight_relevance"],
        "willingness": config.ai_llm.config["heartflow"]["weight_willingness"],
        "social": config.ai_llm.config["heartflow"]["weight_social"],
        "timing": config.ai_llm.config["heartflow"]["weight_timing"],
        "continuity": config.ai_llm.config["heartflow"]["weight_continuity"],
    }

    # å½’ä¸€åŒ–æƒé‡
    weight_sum = sum(weights.values())
    if abs(weight_sum - 1.0) > 1e-6:
        bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šåˆ¤æ–­æƒé‡å’Œä¸ä¸º1 ({weight_sum})ï¼Œå·²è‡ªåŠ¨å½’ä¸€åŒ–")
        weights = {k: v / weight_sum for k, v in weights.items()}

    # ============ çŠ¶æ€ç®¡ç† ============
    chat_states: Dict[int, ChatState] = {}
    persona_cache: Dict[str, str] = {}
    user_state = {}  # ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—çŠ¶æ€
    portrait_updating = set()  # æ­£åœ¨æ›´æ–°ç”»åƒçš„ç”¨æˆ·

    # ============ å·¥å…·å‡½æ•° ============

    def get_chat_state(group_id: int) -> ChatState:
        """è·å–ç¾¤èŠçŠ¶æ€"""
        if group_id not in chat_states:
            chat_states[group_id] = ChatState()

        state = chat_states[group_id]
        today = datetime.date.today().isoformat()
        if state.last_reset_date != today:
            state.last_reset_date = today
            state.energy = min(1.0, state.energy + 0.2)
            bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šç¾¤ {group_id} æ¯æ—¥é‡ç½®ï¼Œç²¾åŠ›æ¢å¤è‡³ {state.energy:.2f}")

        return state

    def get_minutes_since_last_reply(group_id: int) -> int:
        """è·å–è·ç¦»ä¸Šæ¬¡å›å¤çš„åˆ†é’Ÿæ•°"""
        state = get_chat_state(group_id)
        if state.last_reply_time == 0:
            return 999
        return int((time.time() - state.last_reply_time) / 60)

    def update_active_state(group_id: int, user_id: int):
        """æ›´æ–°ä¸»åŠ¨å›å¤çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.last_reply_time = time.time()
        state.total_replies += 1
        state.total_messages += 1
        state.energy = max(0.1, state.energy - config.ai_llm.config["heartflow"]["energy_decay_rate"])
        state.recent_interactions[user_id] = time.time()
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°ä¸»åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def update_passive_state(group_id: int):
        """æ›´æ–°è¢«åŠ¨çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.total_messages += 1
        state.energy = min(1.0, state.energy + config.ai_llm.config["heartflow"]["energy_recovery_rate"])
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°è¢«åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def check_recent_interaction(group_id: int, user_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„äº¤äº’è®°å½•"""
        state = get_chat_state(group_id)
        if user_id not in state.recent_interactions:
            return False

        last_time = state.recent_interactions[user_id]
        time_diff = time.time() - last_time

        if time_diff > config.ai_llm.config["heartflow"]["interaction_timeout"]:
            del state.recent_interactions[user_id]
            return False

        return True

    async def get_persona_prompt(user_id: int) -> str:
        """è·å–ç”¨æˆ·çš„äººæ ¼è®¾å®š"""
        try:
            cache_key = f"persona_{user_id}"
            if cache_key in persona_cache:
                return persona_cache[cache_key]

            user_info = await get_user(user_id)
            chara_file = getattr(user_info, 'chara_file', None)

            if not chara_file or chara_file == "default":
                chara_file = config.ai_llm.config["llm"]["chara_file_name"]

            chara_path = f"./data/system/chara/{chara_file}"
            try:

                persona = await read_chara(user_id, await use_folder_chara(config.ai_llm.config["llm"]["chara_file_name"]))

                if len(persona) > 500:
                    persona = await summarize_persona(persona)

                persona_cache[cache_key] = persona
                return persona
            except FileNotFoundError:
                bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šæœªæ‰¾åˆ°è§’è‰²æ–‡ä»¶ {chara_path}")
                return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šè·å–äººæ ¼è®¾å®šå¤±è´¥ {e}")
            return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"

    async def summarize_persona(original_persona: str) -> str:
        """ç²¾ç®€äººæ ¼è®¾å®š"""
        try:

            prompt = f"""è¯·å°†ä»¥ä¸‹æœºå™¨äººè§’è‰²è®¾å®šæ€»ç»“ä¸ºç®€æ´çš„æ ¸å¿ƒè¦ç‚¹ã€‚
            æ€»ç»“åçš„å†…å®¹åº”è¯¥åœ¨100-200å­—ä»¥å†…ï¼Œçªå‡ºæœ€é‡è¦çš„è§’è‰²ç‰¹ç‚¹ã€‚
            
            åŸå§‹è§’è‰²è®¾å®šï¼š
            {original_persona}"""

            result = await gemma_reply(
                config,
                prompt,
                recursion_times=7
            )

            summarized = result.get("summarized_persona", "")
            if summarized and len(summarized.strip()) > 10:
                bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šäººæ ¼ç²¾ç®€å®Œæˆ {len(original_persona)} -> {len(summarized)}")
                return summarized

            return original_persona
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šç²¾ç®€äººæ ¼å¤±è´¥ {e}")
            return original_persona

    async def judge_should_reply(event: GroupMessageEvent) -> JudgeResult:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å›å¤"""
        try:
            chat_state = get_chat_state(event.group_id)
            persona = await get_persona_prompt(event.user_id)

            group_messages_bg = await get_last_20_and_convert_to_prompt(
                event.group_id, config.ai_llm.config["heartflow"]["context_messages_count"], "gemini", bot
            )


            recent_messages = "\n---\n".join([
                msg.get("text", "") for msg in group_messages_bg[-5:]
                if msg.get("role") in ["user", "model"]
            ]) if group_messages_bg else "æš‚æ— å¯¹è¯å†å²"
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            prompt = f"""ä½ æ˜¯ç¾¤èŠæœºå™¨äººçš„å†³ç­–ç³»ç»Ÿï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨å›å¤ã€‚

                ## æœºå™¨äººè§’è‰²è®¾å®š
                {persona}
                
                ## å½“å‰ç¾¤èŠæƒ…å†µ
                - ç¾¤èŠID: {event.group_id}
                - ç²¾åŠ›æ°´å¹³: {chat_state.energy:.1f}/1.0
                - ä¸Šæ¬¡å‘è¨€: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰
                - å›å¤ç‡: {(chat_state.total_replies / max(1, chat_state.total_messages) * 100):.1f}%
                
                ## æœ€è¿‘å¯¹è¯
                {recent_messages}
                
                ## å¾…åˆ¤æ–­æ¶ˆæ¯
                å‘é€è€…: {event.sender.nickname}
                å†…å®¹: {event.pure_text}
                æ—¶é—´: {datetime.datetime.now().strftime('%H:%M:%S')}
                
                å›å¤é˜ˆå€¼: {reply_threshold}
                è¯·ä»5ä¸ªç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ã€‚"""
            prompt += """

            è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯åšå‡ºåˆ¤æ–­ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

            ç›¸å…³åº¦: 0-10
            æ„æ„¿: 0-10
            ç¤¾äº¤: 0-10
            æ—¶æœº: 0-10
            è¿è´¯: 0-10
            ç†ç”±: è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆåº”æˆ–ä¸åº”å›å¤ï¼ˆç»“åˆè§’è‰²ç‰¹æ€§ï¼‰

            âš ï¸ è¯·ä¸¥æ ¼ä¿æŒè¯¥æ ¼å¼ï¼Œæ¯ä¸ªåˆ†æ•°å­—åªèƒ½å†™ä¸€ä¸ªçº¯æ•°å­—ã€‚
            """

            # æ›¿æ¢ schemaReplyCore â†’ gemma_reply
            result_text = await gemma_reply(
                config,
                prompt,
                group_messages_bg=group_messages_bg
            )
            #print(result_text)
            #print(type(result_text))
            # ä½¿ç”¨æ­£åˆ™è§£æåˆ†æ•°
            import re

            def ext(name):
                m = re.search(rf"(?:{name})\s*[:ï¼š]\s*(\d+)", result_text)
                return float(m.group(1)) if m else 0.0

            relevance = ext("ç›¸å…³åº¦|å†…å®¹ç›¸å…³åº¦|relevance")
            willingness = ext("æ„æ„¿|å›å¤æ„æ„¿|willingness")
            social = ext("ç¤¾äº¤|ç¤¾äº¤é€‚å®œæ€§|social")
            timing = ext("æ—¶æœº|æ—¶æœºæ°å½“æ€§|timing")
            continuity = ext("è¿è´¯|å¯¹è¯è¿è´¯|continuity")


            # æå–ç†ç”±
            reasoning_match = re.search(r"(ç†ç”±|åˆ†æ|åŸå› )[:ï¼š]\s*(.+)", result_text, re.S)
            reasoning = reasoning_match.group(2).strip() if reasoning_match else result_text.strip()

            overall_score = (
                                    relevance * weights["relevance"] +
                                    willingness * weights["willingness"] +
                                    social * weights["social"] +
                                    timing * weights["timing"] +
                                    continuity * weights["continuity"]
                            ) / 10.0

            should_reply = overall_score >= reply_threshold
            #print(should_reply)
            r=JudgeResult(
                relevance=relevance,
                willingness=willingness,
                social=social,
                timing=timing,
                continuity=continuity,
                reasoning=reasoning,
                should_reply=should_reply,
                confidence=overall_score,
                overall_score=overall_score
            )
            print(r)
            return r

        except Exception as e:
            traceback.print_exc()
            bot.logger.error(f"å¿ƒæµåˆ¤æ–­å¼‚å¸¸: {e}")
            return JudgeResult(should_reply=False, reasoning=f"å¼‚å¸¸: {str(e)}")

    # ============ æ¶ˆæ¯å¤„ç†é€»è¾‘ï¼ˆå¤åˆ¶è‡ªåŸæ¡†æ¶ï¼‰============

    async def handle_message(event: GroupMessageEvent, user_info=None):
        """å¤„ç†æ¶ˆæ¯çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰"""
        uid = event.user_id
        if user_info is None:
            user_info = await get_user(event.user_id, event.sender.nickname)

        if uid not in user_state:
            user_state[uid] = {
                "queue": asyncio.Queue(),
                "running": False
            }

        await user_state[uid]["queue"].put(event)

        if user_state[uid]["running"]:
            bot.logger.info(f"ç”¨æˆ·{uid}æ­£åœ¨å¤„ç†ä¸­ï¼Œå·²æ”¾å…¥é˜Ÿåˆ—")
            return

        async def process_user_queue(uid):
            user_state[uid]["running"] = True
            try:
                current_event = await user_state[uid]["queue"].get()
                try:

                    reply_message = await aiReplyCore(
                        current_event.processed_message,
                        current_event.user_id,
                        config,
                        tools=tools,
                        bot=bot,
                        event=current_event,
                        do_not_read_context=True,
                    )

                    if reply_message is None or '' == str(reply_message) or 'Maximum recursion depth' in reply_message:
                        return

                    if "call_send_mface(summary='')" in reply_message:
                        reply_message = reply_message.replace("call_send_mface(summary='')", '')

                    try:
                        tokens_total = count_tokens_approximate(
                            current_event.processed_message[1]['text'],
                            reply_message, user_info.ai_token_record
                        )
                        await update_user(user_id=current_event.user_id, ai_token_record=tokens_total)
                    except:
                        pass

                    await send_text(bot, current_event, config, reply_message.strip())

                except Exception as e:
                    bot.logger.exception(f"ç”¨æˆ· {uid} å¤„ç†å‡ºé”™: {e}")
                finally:
                    user_state[uid]["queue"].task_done()

                    if not user_state[uid]["queue"].empty():
                        asyncio.create_task(process_user_queue(uid))
            finally:
                user_state[uid]["running"] = False

        asyncio.create_task(process_user_queue(uid))

    # ============ äº‹ä»¶å¤„ç†å™¨ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_handler(event: GroupMessageEvent):
        """å¿ƒæµä¸»åŠ¨å›å¤å¤„ç†"""

        # è·³è¿‡å‘½ä»¤å’Œbotè‡ªå·±çš„æ¶ˆæ¯
        if event.pure_text and event.pure_text.startswith("/"):
            return
        if event.user_id == bot.id:
            return
        if not event.pure_text or not event.pure_text.strip():
            return

        # ç™½åå•æ£€æŸ¥
        if config.ai_llm.config["heartflow"]["whitelist_enabled"]:
            if event.group_id not in config.ai_llm.config["heartflow"]["chat_whitelist"]:
                return

        # å¿ƒæµåˆ¤æ–­
        if config.ai_llm.config["heartflow"]["enabled"]:
            try:
                judge_result = await judge_should_reply(event)

                if judge_result.should_reply:
                    bot.logger.info(
                        f"ğŸ”¥ å¿ƒæµè§¦å‘ | ç¾¤:{event.group_id} | è¯„åˆ†:{judge_result.overall_score:.2f}"
                    )

                    # æƒé™æ£€æŸ¥
                    user_info = await get_user(event.user_id, event.sender.nickname)
                    if not user_info.permission >= config.ai_llm.config["core"]["ai_reply_group"]:
                        return

                    if event.group_id in [913122269, 1050663831] and not user_info.permission >= 66:
                        return

                    if not user_info.permission >= config.ai_llm.config["core"]["ai_token_limt"]:
                        if user_info.ai_token_record >= config.ai_llm.config["core"]["ai_token_limt_token"]:
                            return

                    # æ›´æ–°çŠ¶æ€å¹¶å¤„ç†æ¶ˆæ¯
                    update_active_state(event.group_id, event.user_id)
                    await handle_message(event, user_info)
                    return
                else:
                    update_passive_state(event.group_id)

            except Exception as e:
                bot.logger.error(f"å¿ƒæµå¤„ç†å¼‚å¸¸: {e}")



    # ============ ç®¡ç†å‘½ä»¤ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_commands(event: GroupMessageEvent):
        """å¿ƒæµç®¡ç†å‘½ä»¤"""
        if not event.pure_text:
            return

        if event.pure_text == "/heartflow":
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            whitelist_enabled = config.ai_llm.config['heartflow']['whitelist_enabled']
            enabled = config.ai_llm.config['heartflow']['enabled']
            state = get_chat_state(event.group_id)
            status = f"""ğŸ”® å¿ƒæµçŠ¶æ€æŠ¥å‘Š

ğŸ“Š **å½“å‰çŠ¶æ€**
- ç¾¤èŠID: {event.group_id}
- ç²¾åŠ›æ°´å¹³: {state.energy:.2f}/1.0 {'ğŸŸ¢' if state.energy > 0.7 else 'ğŸŸ¡' if state.energy > 0.3 else 'ğŸ”´'}
- ä¸Šæ¬¡å›å¤: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰

ğŸ“ˆ **å†å²ç»Ÿè®¡**
- æ€»æ¶ˆæ¯æ•°: {state.total_messages}
- æ€»å›å¤æ•°: {state.total_replies}
- å›å¤ç‡: {(state.total_replies / max(1, state.total_messages) * 100):.1f}%
- æ´»è·ƒç”¨æˆ·: {len(state.recent_interactions)}äºº

âš™ï¸ **é…ç½®**
- å›å¤é˜ˆå€¼: {reply_threshold}
- ç™½åå•: {'âœ…' if whitelist_enabled else 'âŒ'}
- çŠ¶æ€: {'âœ… å¯ç”¨' if enabled else 'âŒ ç¦ç”¨'}

ğŸ¯ **æƒé‡**
- ç›¸å…³åº¦: {weights['relevance']:.0%}
- æ„æ„¿: {weights['willingness']:.0%}
- ç¤¾äº¤: {weights['social']:.0%}
- æ—¶æœº: {weights['timing']:.0%}
- è¿è´¯: {weights['continuity']:.0%}"""
            await bot.send(event, status)

        elif event.pure_text == "/heartflow_reset":
            if event.group_id in chat_states:
                del chat_states[event.group_id]
            await bot.send(event, "âœ… å¿ƒæµçŠ¶æ€å·²é‡ç½®")

        elif event.pure_text == "/heartflow_cache":
            info = f"ğŸ§  äººæ ¼ç¼“å­˜: {len(persona_cache)}ä¸ª\n\n"
            if persona_cache:
                for key, value in list(persona_cache.items())[:5]:
                    info += f"ğŸ”‘ {key}\nğŸ“„ {value[:80]}...\n\n"
            else:
                info += "ğŸ“­ æ— ç¼“å­˜"
            await bot.send(event, info)

        elif event.pure_text == "/heartflow_cache_clear":
            count = len(persona_cache)
            persona_cache.clear()
            await bot.send(event, f"âœ… å·²æ¸…é™¤ {count} ä¸ªç¼“å­˜")

    bot.logger.info("å¿ƒæµæ’ä»¶å·²åŠ è½½")


