"""
å¿ƒæµä¸»åŠ¨å›å¤æ’ä»¶ - åŸºäºç»“æ„åŒ–è¾“å‡ºçš„æ™ºèƒ½åˆ¤æ–­ç³»ç»Ÿ
å®Œå…¨ç¬¦åˆæ¡†æ¶æ’ä»¶è§„èŒƒï¼Œæ— éœ€ä¿®æ”¹ä¸»ç¨‹åº
"""
import asyncio
import datetime
import time
from typing import Dict
from dataclasses import dataclass, field

from developTools.event.events import GroupMessageEvent, LifecycleMetaEvent
from framework_common.database_util.Group import get_last_20_and_convert_to_prompt
from framework_common.database_util.User import get_user, update_user
from framework_common.database_util.llmDB import delete_latest2_history
from run.ai_llm.service.aiReplyCore import aiReplyCore, send_text, count_tokens_approximate
from run.ai_llm.service.schemaReplyCore import schemaReplyCore


@dataclass
class JudgeResult:
    """åˆ¤æ–­ç»“æœæ•°æ®ç±»"""
    relevance: float = 0.0
    willingness: float = 0.0
    social: float = 0.0
    timing: float = 0.0
    continuity: float = 0.0
    reasoning: str = ""
    should_reply: bool = False
    confidence: float = 0.0
    overall_score: float = 0.0


@dataclass
class ChatState:
    """ç¾¤èŠçŠ¶æ€æ•°æ®ç±»"""
    energy: float = 1.0
    last_reply_time: float = 0.0
    last_reset_date: str = ""
    total_messages: int = 0
    total_replies: int = 0
    recent_interactions: Dict[int, float] = field(default_factory=dict)


def main(bot, config):
    """
    æ­¤æ’ä»¶ä»£ç å‚è€ƒäº†https://github.com/advent259141/Astrbot_plugin_Heartflow
    """
    """å¿ƒæµæ’ä»¶ä¸»å‡½æ•°"""
    # è·å–toolsé…ç½®ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰
    tools = None
    if config.ai_llm.config["llm"]["func_calling"]:
        from framework_common.framework_util.func_map_loader import gemini_func_map, openai_func_map
        if config.ai_llm.config["llm"]["model"] == "gemini":
            tools = gemini_func_map()
        else:
            tools = openai_func_map()

    if config.ai_llm.config["llm"]["è”ç½‘æœç´¢"]:
        if config.ai_llm.config["llm"]["model"] == "gemini":
            if tools is None:
                tools = [{"googleSearch": {}}]
            else:
                tools = [{"googleSearch": {}}, tools]
        else:
            if tools is None:
                tools = [{"type": "function", "function": {"name": "googleSearch"}}]
            else:
                tools = [{"type": "function", "function": {"name": "googleSearch"}}, tools]
    # ============ é…ç½®è¯»å– ============



    # åˆ¤æ–­æƒé‡é…ç½®
    weights = {
        "relevance": config.ai_llm.config["heartflow"]["weight_relevance"],
        "willingness": config.ai_llm.config["heartflow"]["weight_willingness"],
        "social": config.ai_llm.config["heartflow"]["weight_social"],
        "timing": config.ai_llm.config["heartflow"]["weight_timing"],
        "continuity": config.ai_llm.config["heartflow"]["weight_continuity"],
    }

    # å½’ä¸€åŒ–æƒé‡
    weight_sum = sum(weights.values())
    if abs(weight_sum - 1.0) > 1e-6:
        bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šåˆ¤æ–­æƒé‡å’Œä¸ä¸º1 ({weight_sum})ï¼Œå·²è‡ªåŠ¨å½’ä¸€åŒ–")
        weights = {k: v / weight_sum for k, v in weights.items()}

    # ============ çŠ¶æ€ç®¡ç† ============
    chat_states: Dict[int, ChatState] = {}
    persona_cache: Dict[str, str] = {}
    user_state = {}  # ç”¨æˆ·æ¶ˆæ¯é˜Ÿåˆ—çŠ¶æ€
    portrait_updating = set()  # æ­£åœ¨æ›´æ–°ç”»åƒçš„ç”¨æˆ·

    # ============ å·¥å…·å‡½æ•° ============

    def get_chat_state(group_id: int) -> ChatState:
        """è·å–ç¾¤èŠçŠ¶æ€"""
        if group_id not in chat_states:
            chat_states[group_id] = ChatState()

        state = chat_states[group_id]
        today = datetime.date.today().isoformat()
        if state.last_reset_date != today:
            state.last_reset_date = today
            state.energy = min(1.0, state.energy + 0.2)
            bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šç¾¤ {group_id} æ¯æ—¥é‡ç½®ï¼Œç²¾åŠ›æ¢å¤è‡³ {state.energy:.2f}")

        return state

    def get_minutes_since_last_reply(group_id: int) -> int:
        """è·å–è·ç¦»ä¸Šæ¬¡å›å¤çš„åˆ†é’Ÿæ•°"""
        state = get_chat_state(group_id)
        if state.last_reply_time == 0:
            return 999
        return int((time.time() - state.last_reply_time) / 60)

    def update_active_state(group_id: int, user_id: int):
        """æ›´æ–°ä¸»åŠ¨å›å¤çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.last_reply_time = time.time()
        state.total_replies += 1
        state.total_messages += 1
        state.energy = max(0.1, state.energy - config.ai_llm.config["heartflow"]["energy_decay_rate"])
        state.recent_interactions[user_id] = time.time()
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°ä¸»åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def update_passive_state(group_id: int):
        """æ›´æ–°è¢«åŠ¨çŠ¶æ€"""
        state = get_chat_state(group_id)
        state.total_messages += 1
        state.energy = min(1.0, state.energy + config.ai_llm.config["heartflow"]["energy_recovery_rate"])
        bot.logger.debug(f"å¿ƒæµæ’ä»¶ï¼šæ›´æ–°è¢«åŠ¨çŠ¶æ€ | ç¾¤:{group_id} | ç²¾åŠ›:{state.energy:.2f}")

    def check_recent_interaction(group_id: int, user_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„äº¤äº’è®°å½•"""
        state = get_chat_state(group_id)
        if user_id not in state.recent_interactions:
            return False

        last_time = state.recent_interactions[user_id]
        time_diff = time.time() - last_time

        if time_diff > config.ai_llm.config["heartflow"]["interaction_timeout"]:
            del state.recent_interactions[user_id]
            return False

        return True

    async def get_persona_prompt(user_id: int) -> str:
        """è·å–ç”¨æˆ·çš„äººæ ¼è®¾å®š"""
        try:
            cache_key = f"persona_{user_id}"
            if cache_key in persona_cache:
                return persona_cache[cache_key]

            user_info = await get_user(user_id)
            chara_file = getattr(user_info, 'chara_file', None)

            if not chara_file or chara_file == "default":
                chara_file = config.ai_llm.config["llm"]["chara_file_name"]

            chara_path = f"./data/system/{chara_file}"
            try:
                with open(chara_path, 'r', encoding='utf-8') as f:
                    persona = f.read().strip()

                if len(persona) > 500:
                    persona = await summarize_persona(persona)

                persona_cache[cache_key] = persona
                return persona
            except FileNotFoundError:
                bot.logger.warning(f"å¿ƒæµæ’ä»¶ï¼šæœªæ‰¾åˆ°è§’è‰²æ–‡ä»¶ {chara_path}")
                return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šè·å–äººæ ¼è®¾å®šå¤±è´¥ {e}")
            return "é»˜è®¤æ™ºèƒ½åŠ©æ‰‹"

    async def summarize_persona(original_persona: str) -> str:
        """ç²¾ç®€äººæ ¼è®¾å®š"""
        try:
            schema = {
                "type": "object",
                "properties": {
                    "summarized_persona": {
                        "type": "string",
                        "description": "ç²¾ç®€åçš„è§’è‰²è®¾å®šï¼Œä¿ç•™æ ¸å¿ƒç‰¹å¾å’Œè¡Œä¸ºæ–¹å¼ï¼Œ100-200å­—ä»¥å†…"
                    }
                },
                "required": ["summarized_persona"]
            }

            prompt = f"""è¯·å°†ä»¥ä¸‹æœºå™¨äººè§’è‰²è®¾å®šæ€»ç»“ä¸ºç®€æ´çš„æ ¸å¿ƒè¦ç‚¹ã€‚
æ€»ç»“åçš„å†…å®¹åº”è¯¥åœ¨100-200å­—ä»¥å†…ï¼Œçªå‡ºæœ€é‡è¦çš„è§’è‰²ç‰¹ç‚¹ã€‚

åŸå§‹è§’è‰²è®¾å®šï¼š
{original_persona}"""

            result = await schemaReplyCore(
                config, schema, prompt,
                keep_history=False, user_id=0
            )

            summarized = result.get("summarized_persona", "")
            if summarized and len(summarized.strip()) > 10:
                bot.logger.info(f"å¿ƒæµæ’ä»¶ï¼šäººæ ¼ç²¾ç®€å®Œæˆ {len(original_persona)} -> {len(summarized)}")
                return summarized

            return original_persona
        except Exception as e:
            bot.logger.error(f"å¿ƒæµæ’ä»¶ï¼šç²¾ç®€äººæ ¼å¤±è´¥ {e}")
            return original_persona

    async def judge_should_reply(event: GroupMessageEvent) -> JudgeResult:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å›å¤"""
        try:
            chat_state = get_chat_state(event.group_id)
            persona = await get_persona_prompt(event.user_id)

            group_messages_bg = await get_last_20_and_convert_to_prompt(
                event.group_id, config.ai_llm.config["heartflow"]["context_messages_count"], "gemini", bot
            )

            schema = {
                "type": "object",
                "properties": {
                    "relevance": {
                        "type": "number",
                        "description": "å†…å®¹ç›¸å…³åº¦(0-10)ï¼šæ¶ˆæ¯æ˜¯å¦æœ‰è¶£ã€æœ‰ä»·å€¼ã€é€‚åˆå›å¤",
                        "minimum": 0, "maximum": 10
                    },
                    "willingness": {
                        "type": "number",
                        "description": "å›å¤æ„æ„¿(0-10)ï¼šåŸºäºå½“å‰ç²¾åŠ›å’ŒçŠ¶æ€çš„å›å¤æ„æ„¿",
                        "minimum": 0, "maximum": 10
                    },
                    "social": {
                        "type": "number",
                        "description": "ç¤¾äº¤é€‚å®œæ€§(0-10)ï¼šåœ¨å½“å‰ç¾¤èŠæ°›å›´ä¸‹å›å¤æ˜¯å¦åˆé€‚",
                        "minimum": 0, "maximum": 10
                    },
                    "timing": {
                        "type": "number",
                        "description": "æ—¶æœºæ°å½“æ€§(0-10)ï¼šå›å¤æ—¶æœºæ˜¯å¦æ°å½“",
                        "minimum": 0, "maximum": 10
                    },
                    "continuity": {
                        "type": "number",
                        "description": "å¯¹è¯è¿è´¯æ€§(0-10)ï¼šå½“å‰æ¶ˆæ¯ä¸ä¸Šæ¬¡å›å¤çš„å…³è”ç¨‹åº¦",
                        "minimum": 0, "maximum": 10
                    },
                    "reasoning": {
                        "type": "string",
                        "description": "è¯¦ç»†åˆ†æåŸå› "
                    }
                },
                "required": ["relevance", "willingness", "social", "timing", "continuity", "reasoning"]
            }

            recent_messages = "\n---\n".join([
                msg.get("text", "") for msg in group_messages_bg[-5:]
                if msg.get("role") in ["user", "model"]
            ]) if group_messages_bg else "æš‚æ— å¯¹è¯å†å²"
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            prompt = f"""ä½ æ˜¯ç¾¤èŠæœºå™¨äººçš„å†³ç­–ç³»ç»Ÿï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨å›å¤ã€‚

                ## æœºå™¨äººè§’è‰²è®¾å®š
                {persona}
                
                ## å½“å‰ç¾¤èŠæƒ…å†µ
                - ç¾¤èŠID: {event.group_id}
                - ç²¾åŠ›æ°´å¹³: {chat_state.energy:.1f}/1.0
                - ä¸Šæ¬¡å‘è¨€: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰
                - å›å¤ç‡: {(chat_state.total_replies / max(1, chat_state.total_messages) * 100):.1f}%
                
                ## æœ€è¿‘å¯¹è¯
                {recent_messages}
                
                ## å¾…åˆ¤æ–­æ¶ˆæ¯
                å‘é€è€…: {event.sender.nickname}
                å†…å®¹: {event.pure_text}
                æ—¶é—´: {datetime.datetime.now().strftime('%H:%M:%S')}
                
                å›å¤é˜ˆå€¼: {reply_threshold}
                è¯·ä»5ä¸ªç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ã€‚"""

            result = await schemaReplyCore(
                config, schema, prompt,
                keep_history=False, user_id=0,
                group_messages_bg=group_messages_bg
            )

            overall_score = (
                                    result["relevance"] * weights["relevance"] +
                                    result["willingness"] * weights["willingness"] +
                                    result["social"] * weights["social"] +
                                    result["timing"] * weights["timing"] +
                                    result["continuity"] * weights["continuity"]
                            ) / 10.0

            should_reply = overall_score >= reply_threshold

            bot.logger.info(
                f"å¿ƒæµåˆ¤æ–­ | ç¾¤:{event.group_id} | è¯„åˆ†:{overall_score:.2f} | "
                f"å›å¤:{should_reply} | ç†ç”±:{result['reasoning'][:30]}..."
            )

            return JudgeResult(
                relevance=result["relevance"],
                willingness=result["willingness"],
                social=result["social"],
                timing=result["timing"],
                continuity=result["continuity"],
                reasoning=result["reasoning"],
                should_reply=should_reply,
                confidence=overall_score,
                overall_score=overall_score
            )
        except Exception as e:
            bot.logger.error(f"å¿ƒæµåˆ¤æ–­å¼‚å¸¸: {e}")
            return JudgeResult(should_reply=False, reasoning=f"å¼‚å¸¸: {str(e)}")

    # ============ æ¶ˆæ¯å¤„ç†é€»è¾‘ï¼ˆå¤åˆ¶è‡ªåŸæ¡†æ¶ï¼‰============

    async def handle_message(event: GroupMessageEvent, user_info=None):
        """å¤„ç†æ¶ˆæ¯çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»åŸæ¡†æ¶å¤åˆ¶ï¼‰"""
        uid = event.user_id
        if user_info is None:
            user_info = await get_user(event.user_id, event.sender.nickname)

        if uid not in user_state:
            user_state[uid] = {
                "queue": asyncio.Queue(),
                "running": False
            }

        await user_state[uid]["queue"].put(event)

        if user_state[uid]["running"]:
            bot.logger.info(f"ç”¨æˆ·{uid}æ­£åœ¨å¤„ç†ä¸­ï¼Œå·²æ”¾å…¥é˜Ÿåˆ—")
            return

        async def process_user_queue(uid):
            user_state[uid]["running"] = True
            try:
                current_event = await user_state[uid]["queue"].get()
                try:

                    reply_message = await aiReplyCore(
                        current_event.processed_message,
                        current_event.user_id,
                        config,
                        tools=tools,
                        bot=bot,
                        event=current_event,
                        do_not_read_context=True,
                    )

                    if reply_message is None or '' == str(reply_message) or 'Maximum recursion depth' in reply_message:
                        return

                    if "call_send_mface(summary='')" in reply_message:
                        reply_message = reply_message.replace("call_send_mface(summary='')", '')

                    try:
                        tokens_total = count_tokens_approximate(
                            current_event.processed_message[1]['text'],
                            reply_message, user_info.ai_token_record
                        )
                        await update_user(user_id=current_event.user_id, ai_token_record=tokens_total)
                    except:
                        pass

                    await send_text(bot, current_event, config, reply_message.strip())

                except Exception as e:
                    bot.logger.exception(f"ç”¨æˆ· {uid} å¤„ç†å‡ºé”™: {e}")
                finally:
                    user_state[uid]["queue"].task_done()

                    if not user_state[uid]["queue"].empty():
                        asyncio.create_task(process_user_queue(uid))
            finally:
                user_state[uid]["running"] = False

        asyncio.create_task(process_user_queue(uid))

    # ============ äº‹ä»¶å¤„ç†å™¨ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_handler(event: GroupMessageEvent):
        """å¿ƒæµä¸»åŠ¨å›å¤å¤„ç†"""

        # è·³è¿‡å‘½ä»¤å’Œbotè‡ªå·±çš„æ¶ˆæ¯
        if event.pure_text and event.pure_text.startswith("/"):
            return
        if event.user_id == bot.id:
            return
        if not event.pure_text or not event.pure_text.strip():
            return

        # ç™½åå•æ£€æŸ¥
        if config.ai_llm.config["heartflow"]["whitelist_enabled"]:
            if event.group_id not in config.ai_llm.config["heartflow"]["chat_whitelist"]:
                return

        # å¿ƒæµåˆ¤æ–­
        if config.ai_llm.config["heartflow"]["enabled"]:
            try:
                judge_result = await judge_should_reply(event)

                if judge_result.should_reply:
                    bot.logger.info(
                        f"ğŸ”¥ å¿ƒæµè§¦å‘ | ç¾¤:{event.group_id} | è¯„åˆ†:{judge_result.overall_score:.2f}"
                    )

                    # æƒé™æ£€æŸ¥
                    user_info = await get_user(event.user_id, event.sender.nickname)
                    if not user_info.permission >= config.ai_llm.config["core"]["ai_reply_group"]:
                        return

                    if event.group_id in [913122269, 1050663831] and not user_info.permission >= 66:
                        return

                    if not user_info.permission >= config.ai_llm.config["core"]["ai_token_limt"]:
                        if user_info.ai_token_record >= config.ai_llm.config["core"]["ai_token_limt_token"]:
                            return

                    # æ›´æ–°çŠ¶æ€å¹¶å¤„ç†æ¶ˆæ¯
                    update_active_state(event.group_id, event.user_id)
                    await handle_message(event, user_info)
                    return
                else:
                    update_passive_state(event.group_id)

            except Exception as e:
                bot.logger.error(f"å¿ƒæµå¤„ç†å¼‚å¸¸: {e}")



    # ============ ç®¡ç†å‘½ä»¤ ============

    @bot.on(GroupMessageEvent)
    async def heartflow_commands(event: GroupMessageEvent):
        """å¿ƒæµç®¡ç†å‘½ä»¤"""
        if not event.pure_text:
            return

        if event.pure_text == "/heartflow":
            reply_threshold = config.ai_llm.config['heartflow']['reply_threshold']
            whitelist_enabled = config.ai_llm.config['heartflow']['whitelist_enabled']
            enabled = config.ai_llm.config['heartflow']['enabled']
            state = get_chat_state(event.group_id)
            status = f"""ğŸ”® å¿ƒæµçŠ¶æ€æŠ¥å‘Š

ğŸ“Š **å½“å‰çŠ¶æ€**
- ç¾¤èŠID: {event.group_id}
- ç²¾åŠ›æ°´å¹³: {state.energy:.2f}/1.0 {'ğŸŸ¢' if state.energy > 0.7 else 'ğŸŸ¡' if state.energy > 0.3 else 'ğŸ”´'}
- ä¸Šæ¬¡å›å¤: {get_minutes_since_last_reply(event.group_id)}åˆ†é’Ÿå‰

ğŸ“ˆ **å†å²ç»Ÿè®¡**
- æ€»æ¶ˆæ¯æ•°: {state.total_messages}
- æ€»å›å¤æ•°: {state.total_replies}
- å›å¤ç‡: {(state.total_replies / max(1, state.total_messages) * 100):.1f}%
- æ´»è·ƒç”¨æˆ·: {len(state.recent_interactions)}äºº

âš™ï¸ **é…ç½®**
- å›å¤é˜ˆå€¼: {reply_threshold}
- ç™½åå•: {'âœ…' if whitelist_enabled else 'âŒ'}
- çŠ¶æ€: {'âœ… å¯ç”¨' if enabled else 'âŒ ç¦ç”¨'}

ğŸ¯ **æƒé‡**
- ç›¸å…³åº¦: {weights['relevance']:.0%}
- æ„æ„¿: {weights['willingness']:.0%}
- ç¤¾äº¤: {weights['social']:.0%}
- æ—¶æœº: {weights['timing']:.0%}
- è¿è´¯: {weights['continuity']:.0%}"""
            await bot.send(event, status)

        elif event.pure_text == "/heartflow_reset":
            if event.group_id in chat_states:
                del chat_states[event.group_id]
            await bot.send(event, "âœ… å¿ƒæµçŠ¶æ€å·²é‡ç½®")

        elif event.pure_text == "/heartflow_cache":
            info = f"ğŸ§  äººæ ¼ç¼“å­˜: {len(persona_cache)}ä¸ª\n\n"
            if persona_cache:
                for key, value in list(persona_cache.items())[:5]:
                    info += f"ğŸ”‘ {key}\nğŸ“„ {value[:80]}...\n\n"
            else:
                info += "ğŸ“­ æ— ç¼“å­˜"
            await bot.send(event, info)

        elif event.pure_text == "/heartflow_cache_clear":
            count = len(persona_cache)
            persona_cache.clear()
            await bot.send(event, f"âœ… å·²æ¸…é™¤ {count} ä¸ªç¼“å­˜")

    bot.logger.info("å¿ƒæµæ’ä»¶å·²åŠ è½½")


